import os
import pandas as pd
import numpy as np
import glob
import geopandas as gpd
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.cluster import KMeans, DBSCAN
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.spatial.distance import cdist
import folium
from folium.plugins import HeatMap, MarkerCluster
import time
from datetime import datetime, timedelta
import warnings
import psutil
import gc
import logging
import hashlib
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
import json
from typing import Dict, List, Tuple, Optional
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HK_POI_Traffic_Ultimate_Analysis:
    
    def _process_district_features(self, district_data):
        """å•åŒºåŸŸç‰¹å¾å¤„ç†ï¼ˆç”¨äºå¹¶è¡Œï¼‰ï¼Œå¢å¼ºç©ºæ•°æ®å¤„ç†å’Œå¼‚å¸¸æ—¥å¿—"""
        # ç©ºæ•°æ®æ£€æŸ¥
        if district_data.empty:
            logger.warning("è·³è¿‡ç©ºåŒºåŸŸæ•°æ®çš„ç‰¹å¾è®¡ç®—")
            return district_data
        
        try:
            # å®‰å…¨è·å–åŒºåŸŸå
            district = district_data['district'].iloc[0] if 'district' in district_data.columns else "æœªçŸ¥åŒºåŸŸ"
            
            # è®¡ç®—å°æ—¶è¶‹åŠ¿ç‰¹å¾ï¼ˆæ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹ï¼Œé¿å…åç»­ç±»å‹é”™è¯¯ï¼‰
            district_data['volume_hour_trend'] = district_data.groupby('hour')['volume_mean'].transform(
                lambda x: x.rolling(window=3, min_periods=1).mean()
            ).astype(np.float32)  # é™åˆ¶ç²¾åº¦ï¼ŒèŠ‚çœå†…å­˜
            
            district_data['speed_hour_trend'] = district_data.groupby('hour')['speed_mean'].transform(
                lambda x: x.rolling(window=3, min_periods=1).mean()
            ).astype(np.float32)
            
            logger.debug(f"å®ŒæˆåŒºåŸŸ {district} çš„ç‰¹å¾è®¡ç®—")
            return district_data
        
        except Exception as e:
            # å¼‚å¸¸æ—¶å°½å¯èƒ½è·å–åŒºåŸŸå
            district_name = "æœªçŸ¥åŒºåŸŸ"
            try:
                if 'district' in district_data.columns and not district_data.empty:
                    district_name = district_data['district'].iloc[0]
            except:
                pass
            logger.warning(f"åŒºåŸŸ {district_name} ç‰¹å¾è®¡ç®—å¤±è´¥: {str(e)}")
            return district_data  # è¿”å›åŸå§‹æ•°æ®é¿å…ä¸­æ–­

    # ä¿®æ”¹advanced_feature_engineeringä¸­çš„ç‰¹å¾è®¡ç®—éƒ¨åˆ†ï¼ŒåŠ å…¥å¹¶è¡Œé€»è¾‘ï¼ˆæ›¿æ¢è¡ç”Ÿç‰¹å¾è®¡ç®—åçš„æ­¥éª¤ï¼‰
    # 2. è¡ç”Ÿç‰¹å¾è®¡ç®—ï¼ˆåŠ å…¥å¹¶è¡Œå¤„ç†é€»è¾‘ï¼‰
    if self.parallel_processing and len(traffic_clean['district'].unique()) > 1:
        # æŒ‰åŒºåŸŸæ‹†åˆ†æ•°æ®
        district_groups = [group for _, group in traffic_clean.groupby('district')]
        
        # å¹¶è¡Œå¤„ç†å„åŒºåŸŸï¼ˆé™åˆ¶æœ€å¤§è¿›ç¨‹æ•°ä¸ºCPUæ ¸å¿ƒæ•°-1ï¼Œé¿å…èµ„æºè€—å°½ï¼‰
        with ProcessPoolExecutor(max_workers=max(1, mp.cpu_count() - 1)) as executor:
            results = list(executor.map(self._process_district_features, district_groups))
        
        # åˆå¹¶ç»“æœ
        traffic_clean = pd.concat(results, ignore_index=True)
        logger.info(f"å¹¶è¡Œå¤„ç†å®Œæˆ: {len(district_groups)} ä¸ªåŒºåŸŸ")
    else:
        # å•çº¿ç¨‹å¤„ç†
        traffic_clean = self._process_district_features(traffic_clean)
    
        # åç»­ç‰¹å¾å¤„ç†...
        return traffic_clean

    def __init__(self, traffic_base_path, restaurant_data_path, output_path, config_path=None):
        """ç»ˆæä¼˜åŒ–ç‰ˆæœ¬ - æ”¯æŒé…ç½®æ–‡ä»¶"""
        self.traffic_base_path = traffic_base_path
        self.restaurant_data_path = restaurant_data_path
        self.output_path = output_path
        
        # åŠ è½½é…ç½®ï¼ˆé»˜è®¤å€¼ + é…ç½®æ–‡ä»¶è¦†ç›–ï¼‰
        self.config = {
            'chunksize': 50000,
            'max_memory_threshold': 85,
            'use_dask': True,
            'parallel_processing': True,
            'log_level': 'INFO',
            'aggregation_methods': {'volume': ['mean', 'count'], 'speed': 'mean'}
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                self.config.update(json.load(f))
        
        # åº”ç”¨é…ç½®
        self.chunksize = self.config['chunksize']
        self.max_memory_threshold = self.config['max_memory_threshold']
        self.use_dask = self.config['use_dask']
        self.parallel_processing = self.config['parallel_processing']
        
        # é…ç½®æ—¥å¿—
        log_level = getattr(logging, self.config['log_level'].upper(), logging.INFO)
        logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # å‰©ä½™åˆå§‹åŒ–é€»è¾‘ä¸å˜...
        self.district_mapping = self.create_comprehensive_district_mapping()
        self.detector_district_mapping = self.create_comprehensive_detector_mapping()
        self.hk_districts_info = self.get_real_hk_district_info()
        
        if not os.path.exists(output_path):
            os.makedirs(output_path)
        
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        
        self.logger.info("ç»ˆæä¼˜åŒ–ç‰ˆåˆ†æç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory = psutil.virtual_memory()
        return {
            'total_gb': memory.total / (1024**3),
            'used_gb': memory.used / (1024**3),
            'available_gb': memory.available / (1024**3),
            'percent': memory.percent
        }


    @staticmethod
    def memory_safe_operation(operation_name):
        """å†…å­˜å®‰å…¨æ“ä½œè£…é¥°å™¨"""
        def decorator(func):
            def wrapper(self, *args, **kwargs):
                start_memory = self.get_memory_usage()
                logger.info(f"å¼€å§‹ {operation_name} - å½“å‰å†…å­˜: {start_memory['percent']:.1f}%")
                
                result = func(self, *args, **kwargs)
                
                end_memory = self.get_memory_usage()
                memory_used = end_memory['used_gb'] - start_memory['used_gb']
                logger.info(f"å®Œæˆ {operation_name} - å†…å­˜å˜åŒ–: {memory_used:+.2f}GB, å½“å‰: {end_memory['percent']:.1f}%")
                
                # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå¼ºåˆ¶åƒåœ¾å›æ”¶
                if end_memory['percent'] > 80:
                    gc.collect()
                    logger.warning("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ‰§è¡Œå¼ºåˆ¶åƒåœ¾å›æ”¶")
                
                return result
            return wrapper
        return decorator

    def optimize_dataframe_memory(self, df):
        """æ·±åº¦ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨"""
        start_mem = df.memory_usage(deep=True).sum() / 1024**2
        
        # æ•°å€¼åˆ—ç±»å‹ä¼˜åŒ–
        for col in df.select_dtypes(include=[np.number]).columns:
            col_min = df[col].min()
            col_max = df[col].max()
            
            # æ•´æ•°ä¼˜åŒ–
            if pd.api.types.is_integer_dtype(df[col]):
                if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                else:
                    df[col] = df[col].astype(np.int64)
            # æµ®ç‚¹æ•°ä¼˜åŒ–
            elif pd.api.types.is_float_dtype(df[col]):
                df[col] = df[col].astype(np.float32)
        
        # å­—ç¬¦ä¸²åˆ—ä¼˜åŒ–
        for col in df.select_dtypes(include=['object']).columns:
            num_unique = df[col].nunique()
            num_total = len(df[col])
            if num_unique / num_total < 0.5:
                df[col] = df[col].astype('category')
        
        end_mem = df.memory_usage(deep=True).sum() / 1024**2
        reduction = 100 * (start_mem - end_mem) / start_mem
        logger.info(f"å†…å­˜ä¼˜åŒ–: {start_mem:.2f} MB -> {end_mem:.2f} MB (-{reduction:.1f}%)")
        
        return df

    def get_real_hk_district_info(self):
        """è·å–çœŸå®çš„é¦™æ¸¯è¡Œæ”¿åŒºä¿¡æ¯"""
        district_info = {
            'district_english': [
                'Central and Western', 'Wan Chai', 'Eastern', 'Southern',
                'Yau Tsim Mong', 'Sham Shui Po', 'Kowloon City', 'Wong Tai Sin',
                'Kwun Tong', 'Kwai Tsing', 'Tsuen Wan', 'Tuen Mun', 
                'Yuen Long', 'North', 'Tai Po', 'Sha Tin', 'Sai Kung', 'Islands'
            ],
            'district_chinese': [
                'ä¸­è¥¿å€', 'ç£ä»”å€', 'æ±å€', 'å—å€', 'æ²¹å°–æ—ºå€', 'æ·±æ°´åŸ—å€', 
                'ä¹é¾åŸå€', 'é»ƒå¤§ä»™å€', 'è§€å¡˜å€', 'è‘µé’å€', 'èƒç£å€', 'å±¯é–€å€',
                'å…ƒæœ—å€', 'åŒ—å€', 'å¤§åŸ”å€', 'æ²™ç”°å€', 'è¥¿è²¢å€', 'é›¢å³¶å€'
            ],
            'area_sqkm': [
                12.44, 10.02, 18.56, 38.85, 6.99, 9.35, 10.02, 9.30, 
                11.27, 23.34, 61.71, 84.45, 138.46, 137.31, 148.18, 
                69.38, 136.39, 175.12
            ],
            'population_2023': [
                243266, 180123, 529603, 274994, 342970, 405869, 418732, 
                425235, 662542, 520572, 320094, 506879, 662100, 311467, 
                303926, 659794, 461864, 185282
            ],
            'population_density': [
                19555, 17976, 28535, 7078, 49066, 43408, 41790, 
                45724, 58788, 22304, 5188, 6003, 4782, 2268, 
                2051, 9510, 3386, 1058
            ],
            'gdp_per_capita_hkd': [
                856000, 782000, 345000, 412000, 328000, 215000, 285000,
                235000, 198000, 185000, 275000, 168000, 155000, 145000,
                165000, 245000, 185000, 135000
            ],
            'commercial_importance': [
                0.95, 0.90, 0.75, 0.60, 0.92, 0.65, 0.70,
                0.55, 0.68, 0.62, 0.58, 0.45, 0.40, 0.35,
                0.38, 0.52, 0.42, 0.30
            ]
        }
        
        df = pd.DataFrame(district_info)
        df['economic_index'] = df['gdp_per_capita_hkd'] * df['commercial_importance'] / 100000
        
        return df

    def create_comprehensive_district_mapping(self):
        """åˆ›å»ºå®Œæ•´çš„é¦™æ¸¯è¡Œæ”¿åŒºåç§°æ˜ å°„"""
        district_mapping = {
            # ä¸­æ–‡åç§°æ˜ å°„
            'ä¸­è¥¿å€': 'Central and Western', 'ä¸­è¥¿åŒº': 'Central and Western',
            'ç£ä»”å€': 'Wan Chai', 'æ¹¾ä»”åŒº': 'Wan Chai',
            'æ±å€': 'Eastern', 'ä¸œåŒº': 'Eastern',
            'å—å€': 'Southern', 'å—åŒº': 'Southern',
            'æ²¹å°–æ—ºå€': 'Yau Tsim Mong', 'æ²¹å°–æ—ºåŒº': 'Yau Tsim Mong',
            'æ·±æ°´åŸ—å€': 'Sham Shui Po', 'æ·±æ°´åŸ—åŒº': 'Sham Shui Po',
            'ä¹é¾åŸå€': 'Kowloon City', 'ä¹é¾™åŸåŒº': 'Kowloon City',
            'é»ƒå¤§ä»™å€': 'Wong Tai Sin', 'é»„å¤§ä»™åŒº': 'Wong Tai Sin',
            'è§€å¡˜å€': 'Kwun Tong', 'è§‚å¡˜åŒº': 'Kwun Tong',
            'è‘µé’å€': 'Kwai Tsing', 'è‘µé’åŒº': 'Kwai Tsing',
            'èƒç£å€': 'Tsuen Wan', 'èƒæ¹¾åŒº': 'Tsuen Wan',
            'å±¯é–€å€': 'Tuen Mun', 'å±¯é—¨åŒº': 'Tuen Mun',
            'å…ƒæœ—å€': 'Yuen Long', 'å…ƒæœ—åŒº': 'Yuen Long',
            'åŒ—å€': 'North', 'åŒ—åŒº': 'North',
            'å¤§åŸ”å€': 'Tai Po', 'å¤§åŸ”åŒº': 'Tai Po',
            'æ²™ç”°å€': 'Sha Tin', 'æ²™ç”°åŒº': 'Sha Tin',
            'è¥¿è²¢å€': 'Sai Kung', 'è¥¿è´¡åŒº': 'Sai Kung',
            'é›¢å³¶å€': 'Islands', 'ç¦»å²›åŒº': 'Islands',
            # è‹±æ–‡åç§°æ˜ å°„
            'Central and Western': 'Central and Western',
            'Wan Chai': 'Wan Chai', 'Wan Chai District': 'Wan Chai',
            'Eastern': 'Eastern', 'Eastern District': 'Eastern',
            'Southern': 'Southern', 'Southern District': 'Southern',
            'Yau Tsim Mong': 'Yau Tsim Mong',
            'Sham Shui Po': 'Sham Shui Po',
            'Kowloon City': 'Kowloon City',
            'Wong Tai Sin': 'Wong Tai Sin',
            'Kwun Tong': 'Kwun Tong',
            'Kwai Tsing': 'Kwai Tsing',
            'Tsuen Wan': 'Tsuen Wan',
            'Tuen Mun': 'Tuen Mun',
            'Yuen Long': 'Yuen Long',
            'North': 'North', 'North District': 'North',
            'Tai Po': 'Tai Po',
            'Sha Tin': 'Sha Tin',
            'Sai Kung': 'Sai Kung',
            'Islands': 'Islands', 'Islands District': 'Islands'
        }
        return district_mapping

    def create_comprehensive_detector_mapping(self):
        """åŸºäºé¦™æ¸¯å®é™…äº¤é€šæ£€æµ‹å™¨åˆ†å¸ƒåˆ›å»ºå®Œæ•´æ˜ å°„"""
        detector_mapping = {
            # ä¸­è¥¿åŒº Central and Western
            'AID': 'Central and Western', 'CID': 'Central and Western', 'CWD': 'Central and Western',
            'C01': 'Central and Western', 'C02': 'Central and Western', 'C03': 'Central and Western',
            # æ¹¾ä»”åŒº Wan Chai
            'WCD': 'Wan Chai', 'WAD': 'Wan Chai', 'WHD': 'Wan Chai', 'W01': 'Wan Chai', 'W02': 'Wan Chai',
            # ä¸œåŒº Eastern
            'EAD': 'Eastern', 'EID': 'Eastern', 'ECD': 'Eastern', 'E01': 'Eastern', 'E02': 'Eastern',
            'E03': 'Eastern', 'E04': 'Eastern',
            # å—åŒº Southern
            'SOD': 'Southern', 'SID': 'Southern', 'SAD': 'Southern', 'S01': 'Southern', 'S02': 'Southern',
            # æ²¹å°–æ—ºåŒº Yau Tsim Mong
            'YTD': 'Yau Tsim Mong', 'YMD': 'Yau Tsim Mong', 'YCD': 'Yau Tsim Mong', 'Y01': 'Yau Tsim Mong',
            'Y02': 'Yau Tsim Mong', 'Y03': 'Yau Tsim Mong', 'Y04': 'Yau Tsim Mong',
            # æ·±æ°´åŸ—åŒº Sham Shui Po
            'SSD': 'Sham Shui Po', 'SPD': 'Sham Shui Po', 'SBD': 'Sham Shui Po', 'SS01': 'Sham Shui Po',
            'SS02': 'Sham Shui Po',
            # ä¹é¾™åŸåŒº Kowloon City
            'KCD': 'Kowloon City', 'KLD': 'Kowloon City', 'KWD': 'Kowloon City', 'KC01': 'Kowloon City',
            'KC02': 'Kowloon City', 'KC03': 'Kowloon City',
            # é»„å¤§ä»™åŒº Wong Tai Sin
            'WTD': 'Wong Tai Sin', 'WSD': 'Wong Tai Sin', 'WKD': 'Wong Tai Sin', 'WT01': 'Wong Tai Sin',
            'WT02': 'Wong Tai Sin',
            # è§‚å¡˜åŒº Kwun Tong
            'KTD': 'Kwun Tong', 'KWD': 'Kwun Tong', 'KPD': 'Kwun Tong', 'KT01': 'Kwun Tong', 'KT02': 'Kwun Tong',
            'KT03': 'Kwun Tong', 'KT04': 'Kwun Tong',
            # è‘µé’åŒº Kwai Tsing
            'KWD': 'Kwai Tsing', 'KGD': 'Kwai Tsing', 'KBD': 'Kwai Tsing', 'KW01': 'Kwai Tsing', 'KW02': 'Kwai Tsing',
            # èƒæ¹¾åŒº Tsuen Wan
            'TWD': 'Tsuen Wan', 'TSD': 'Tsuen Wan', 'TND': 'Tsuen Wan', 'TW01': 'Tsuen Wan', 'TW02': 'Tsuen Wan',
            # å±¯é—¨åŒº Tuen Mun
            'TMD': 'Tuen Mun', 'TUD': 'Tuen Mun', 'TTD': 'Tuen Mun', 'TM01': 'Tuen Mun', 'TM02': 'Tuen Mun',
            'TM03': 'Tuen Mun',
            # å…ƒæœ—åŒº Yuen Long
            'YLD': 'Yuen Long', 'YUD': 'Yuen Long', 'YND': 'Yuen Long', 'YL01': 'Yuen Long', 'YL02': 'Yuen Long',
            'YL03': 'Yuen Long',
            # åŒ—åŒº North
            'NOD': 'North', 'NRD': 'North', 'NSD': 'North', 'NO01': 'North', 'NO02': 'North',
            # å¤§åŸ”åŒº Tai Po
            'TPD': 'Tai Po', 'TAD': 'Tai Po', 'TBD': 'Tai Po', 'TP01': 'Tai Po', 'TP02': 'Tai Po',
            # æ²™ç”°åŒº Sha Tin
            'STD': 'Sha Tin', 'SHD': 'Sha Tin', 'SID': 'Sha Tin', 'ST01': 'Sha Tin', 'ST02': 'Sha Tin',
            'ST03': 'Sha Tin', 'ST04': 'Sha Tin',
            # è¥¿è´¡åŒº Sai Kung
            'SKD': 'Sai Kung', 'SGD': 'Sai Kung', 'SBD': 'Sai Kung', 'SK01': 'Sai Kung', 'SK02': 'Sai Kung',
            # ç¦»å²›åŒº Islands
            'ISD': 'Islands', 'ILD': 'Islands', 'ITD': 'Islands', 'IS01': 'Islands', 'IS02': 'Islands'
        }
        return detector_mapping

    @memory_safe_operation("äº¤é€šæ•°æ®åŠ è½½")
    def load_traffic_data_optimized(self):
        """ä¼˜åŒ–ç‰ˆäº¤é€šæ•°æ®åŠ è½½ - è§£å†³å†…å­˜é—®é¢˜"""
        logger.info("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆäº¤é€šæ•°æ®åŠ è½½...")
        
        traffic_pattern = os.path.join(self.traffic_base_path, "**", "*.csv")
        traffic_files = glob.glob(traffic_pattern, recursive=True)
        
        if not traffic_files:
            raise FileNotFoundError(f"åœ¨ {self.traffic_base_path} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        
        logger.info(f"æ‰¾åˆ° {len(traffic_files)} ä¸ªäº¤é€šæ•°æ®æ–‡ä»¶")
        
        if self.use_dask and len(traffic_files) > 3:
            return self.load_traffic_with_dask(traffic_files)
        else:
            return self.load_traffic_with_pandas(traffic_files)

    def load_traffic_with_dask(self, traffic_files):
        """ä½¿ç”¨DaskåŠ è½½å¤§äº¤é€šæ•°æ®"""
        logger.info("ä½¿ç”¨Daskå¹¶è¡Œå¤„ç†äº¤é€šæ•°æ®...")
        
        all_aggregated_results = []
        
        for i, file_path in enumerate(traffic_files):
            try:
                logger.info(f"å¤„ç†äº¤é€šæ–‡ä»¶ {i+1}/{len(traffic_files)}: {os.path.basename(file_path)}")
                
                # ä½¿ç”¨Daskè¯»å–
                ddf = dd.read_csv(
                    file_path, 
                    encoding='utf-8-sig',
                    assume_missing=True,
                    blocksize="32MB"  # æ›´å°çš„å—å¤§å°ä»¥é€‚åº”å†…å­˜
                )
                
                # å¤„ç†æ•°æ®
                processed_ddf = self.process_traffic_dask(ddf)
                
                # è®¡ç®—èšåˆç»“æœ
                aggregated = processed_ddf.groupby(['district', 'hour']).agg({
                    'volume': ['mean', 'count'],
                    'speed': 'mean'
                }).compute()
                
                # æ‰å¹³åŒ–åˆ—å
                aggregated.columns = ['volume_mean', 'record_count', 'speed_mean']
                aggregated = aggregated.reset_index()
                
                all_aggregated_results.append(aggregated)
                logger.info(f"  âœ… æˆåŠŸå¤„ç†: {len(aggregated):,} æ¡èšåˆè®°å½•")
                
                # æ¸…ç†å†…å­˜
                del ddf, processed_ddf, aggregated
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        if all_aggregated_results:
            # åˆå¹¶æ‰€æœ‰èšåˆç»“æœ
            combined_aggregated = pd.concat(all_aggregated_results, ignore_index=True)
            
            # æœ€ç»ˆèšåˆ
            final_aggregated = combined_aggregated.groupby(['district', 'hour']).agg({
                'volume_mean': 'mean',
                'record_count': 'sum',
                'speed_mean': 'mean'
            }).reset_index()
            
            final_aggregated = self.optimize_dataframe_memory(final_aggregated)
            
            # æ–°å¢ï¼šèšåˆåæ•°æ®æ ¡éªŒ
            self.validate_aggregated_traffic_data(final_aggregated)
            
            logger.info(f"âœ… äº¤é€šæ•°æ®åŠ è½½å®Œæˆ: {len(final_aggregated):,} æ¡èšåˆè®°å½•")
            return final_aggregated
        else:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„äº¤é€šæ•°æ®")

    def load_traffic_with_pandas(self, traffic_files):
        """ä½¿ç”¨PandasåŠ è½½äº¤é€šæ•°æ®ï¼ˆé€‚ç”¨äºå°æ•°æ®é‡ï¼‰"""
        logger.info("ä½¿ç”¨Pandaså¤„ç†äº¤é€šæ•°æ®...")
        
        all_data = []
        
        for i, file_path in enumerate(traffic_files):
            try:
                logger.info(f"å¤„ç†äº¤é€šæ–‡ä»¶ {i+1}/{len(traffic_files)}: {os.path.basename(file_path)}")
                
                file_data = self.process_traffic_file_safely(file_path)
                if file_data is not None and not file_data.empty:
                    all_data.append(file_data)
                    logger.info(f"  âœ… æˆåŠŸåŠ è½½: {len(file_data):,} æ¡è®°å½•")
                
                # å†…å­˜ç®¡ç†
                if (i + 1) % 2 == 0:
                    gc.collect()
                    memory_info = self.get_memory_usage()
                    if memory_info['percent'] > self.max_memory_threshold:
                        logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜ ({memory_info['percent']:.1f}%)ï¼Œæš‚åœå¤„ç†")
                        time.sleep(5)
                        gc.collect()
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            combined_data = self.optimize_dataframe_memory(combined_data)
            
            logger.info(f"âœ… äº¤é€šæ•°æ®åŠ è½½å®Œæˆ: {len(combined_data):,} æ¡è®°å½•")
            
            # éªŒè¯æ•°æ®è´¨é‡
            self.validate_traffic_data(combined_data)
            
            return combined_data
        else:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„äº¤é€šæ•°æ®")

    def process_traffic_dask(self, ddf):
        """ä½¿ç”¨Daskå¤„ç†äº¤é€šæ•°æ®"""
        # é€‰æ‹©éœ€è¦çš„åˆ—
        required_cols = ['detector_id', 'volume', 'speed', 'period_from']
        available_cols = [col for col in required_cols if col in ddf.columns]
        ddf = ddf[available_cols]
        
        # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…æ´—
        if 'volume' in ddf.columns:
            ddf['volume'] = dd.to_numeric(ddf['volume'], errors='coerce')
        if 'speed' in ddf.columns:
            ddf['speed'] = dd.to_numeric(ddf['speed'], errors='coerce')
        
        # æ•°æ®è¿‡æ»¤
        ddf = ddf[ddf['volume'].notnull() & ddf['speed'].notnull()]
        ddf = ddf[(ddf['volume'] >= 0) & (ddf['volume'] <= 10000)]
        ddf = ddf[(ddf['speed'] >= 0) & (ddf['speed'] <= 120)]
        ddf = ddf[ddf['detector_id'].notnull()]
        
        # æ—¶é—´å¤„ç†
        ddf['hour'] = ddf['period_from'].apply(
            lambda x: self.extract_hour_from_period(x), 
            meta=('hour', 'int32')
        )
        
        # åŒºåŸŸæ˜ å°„
        ddf['district'] = ddf['detector_id'].apply(
            lambda x: self.fixed_detector_mapping(x),
            meta=('district', 'object')
        )
        
        return ddf

    def process_traffic_file_safely(self, file_path):
        """å®‰å…¨å¤„ç†äº¤é€šæ•°æ®æ–‡ä»¶"""
        try:
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            
            if file_size_mb > 100:  # é™ä½å¤§æ–‡ä»¶é˜ˆå€¼
                return self.process_large_traffic_file(file_path)
            else:
                return self.process_small_traffic_file(file_path)
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return None

    def process_large_traffic_file(self, file_path):
        """å¤„ç†å¤§å‹äº¤é€šæ•°æ®æ–‡ä»¶"""
        logger.info("  ä½¿ç”¨åˆ†å—å¤„ç†å¤§å‹æ–‡ä»¶...")
        
        chunksize = self.chunksize
        aggregated_chunks = []
        chunk_count = 0
        
        try:
            for chunk in pd.read_csv(file_path, encoding='utf-8-sig', 
                                   chunksize=chunksize, low_memory=False):
                chunk_count += 1
                
                chunk_processed = self.process_traffic_chunk_fixed(chunk)
                if chunk_processed is not None and not chunk_processed.empty:
                    aggregated_chunks.append(chunk_processed)
                
                # æ›´é¢‘ç¹çš„å†…å­˜ç®¡ç†
                if chunk_count % 10 == 0:
                    gc.collect()
                    memory_info = self.get_memory_usage()
                    if memory_info['percent'] > self.max_memory_threshold:
                        logger.warning(f"å†…å­˜å‹åŠ›å¤§ï¼Œæš‚åœå¤„ç†...")
                        time.sleep(2)
                
                if chunk_count % 50 == 0:
                    logger.info(f"    å·²å¤„ç† {chunk_count} ä¸ªæ•°æ®å—...")
            
            if aggregated_chunks:
                file_data = pd.concat(aggregated_chunks, ignore_index=True)
                file_data = self.optimize_dataframe_memory(file_data)
                return file_data
            else:
                return None
                
        except Exception as e:
            logger.error(f"åˆ†å—å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

    def process_small_traffic_file(self, file_path):
        """å¤„ç†å°å‹äº¤é€šæ•°æ®æ–‡ä»¶"""
        logger.info("  ç›´æ¥å¤„ç†å°å‹æ–‡ä»¶...")
        
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)
            processed_df = self.process_traffic_chunk_fixed(df)
            if processed_df is not None:
                processed_df = self.optimize_dataframe_memory(processed_df)
            return processed_df
        except Exception as e:
            logger.error(f"ç›´æ¥å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

    def process_traffic_chunk_fixed(self, chunk):
        """ä¿®å¤ç‰ˆäº¤é€šæ•°æ®å—å¤„ç†"""
        try:
            required_cols = ['detector_id', 'volume', 'speed', 'period_from']
            available_cols = [col for col in required_cols if col in chunk.columns]
            
            if not available_cols:
                return None
            
            df = chunk[available_cols].copy()
            
            # æ•°æ®ç±»å‹ä¼˜åŒ–
            if 'volume' in df.columns:
                df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
            if 'speed' in df.columns:
                df['speed'] = pd.to_numeric(df['speed'], errors='coerce')
            
            # æ•°æ®æ¸…æ´—
            df = df[df['volume'].notna() & df['speed'].notna()]
            df = df[(df['volume'] >= 0) & (df['volume'] <= 10000)]
            df = df[(df['speed'] >= 0) & (df['speed'] <= 120)]
            df = df[df['detector_id'].notna()]
            
            if df.empty:
                return None
            
            # æ—¶é—´å¤„ç†
            df = self.process_time_info_fixed(df)
            
            # åŒºåŸŸæ˜ å°„
            df['district'] = df['detector_id'].apply(self.fixed_detector_mapping)
            
            return df
            
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®å—æ—¶å‡ºé”™: {e}")
            return None

    def fixed_detector_mapping(self, detector_id):
        """å‘é‡åŒ–ä¼˜åŒ–çš„æ£€æµ‹å™¨åŒºåŸŸæ˜ å°„ï¼Œä¼˜åŒ–å•ä¸ªIDå¤„ç†æ•ˆç‡"""
        # å¤„ç†å•ä¸ªæ£€æµ‹å™¨IDï¼ˆéSeriesç±»å‹ï¼‰
        if isinstance(detector_id, str):
            return self._map_single_detector(detector_id)
        
        # å¤„ç†Seriesç±»å‹ï¼ˆæ‰¹é‡æ˜ å°„ï¼‰
        detector_upper = detector_id.astype(str).str.upper()
        districts = pd.Series(['Unknown'] * len(detector_upper), index=detector_upper.index)
        
        # å‘é‡åŒ–åŒ¹é…å‰ç¼€
        for prefix, district in self.detector_district_mapping.items():
            mask = detector_upper.str.startswith(prefix)
            districts[mask] = district
        
        # å¤„ç†æœªåŒ¹é…çš„ID
        unknown_mask = districts == 'Unknown'
        if unknown_mask.any():
            unknown_ids = detector_upper[unknown_mask]
            hash_vals = unknown_ids.apply(lambda x: int(hashlib.md5(x.encode()).hexdigest(), 16) % 10000)
            
            districts_list = [
                'Central and Western', 'Wan Chai', 'Eastern', 'Southern',
                'Yau Tsim Mong', 'Sham Shui Po', 'Kowloon City', 'Wong Tai Sin',
                'Kwun Tong', 'Kwai Tsing', 'Tsuen Wan', 'Tuen Mun', 
                'Yuen Long', 'North', 'Tai Po', 'Sha Tin', 'Sai Kung', 'Islands'
            ]
            weights = np.array([
                0.08, 0.07, 0.09, 0.06, 0.10, 0.08, 0.07, 0.06,
                0.09, 0.06, 0.05, 0.05, 0.04, 0.04, 0.03, 0.04, 0.03, 0.02
            ])
            weights = weights / weights.sum()
            
            np.random.seed(42)  # å›ºå®šç§å­ç¡®ä¿ç»“æœå¯å¤ç°
            random_choices = np.random.choice(districts_list, size=len(unknown_ids), p=weights)
            districts[unknown_mask] = random_choices
        
        return districts.values if len(districts) > 1 else districts.iloc[0]

    def _map_single_detector(self, detector_id):
        """å•ç‹¬å¤„ç†å•ä¸ªæ£€æµ‹å™¨IDï¼Œé¿å…ä¸å¿…è¦çš„Seriesè½¬æ¢"""
        detector_upper = detector_id.upper()
        
        # å‰ç¼€åŒ¹é…
        for prefix, district in self.detector_district_mapping.items():
            if detector_upper.startswith(prefix):
                return district
        
        # æœªåŒ¹é…IDçš„ç¡®å®šæ€§æ˜ å°„
        districts_list = [
            'Central and Western', 'Wan Chai', 'Eastern', 'Southern',
            'Yau Tsim Mong', 'Sham Shui Po', 'Kowloon City', 'Wong Tai Sin',
            'Kwun Tong', 'Kwai Tsing', 'Tsuen Wan', 'Tuen Mun', 
            'Yuen Long', 'North', 'Tai Po', 'Sha Tin', 'Sai Kung', 'Islands'
        ]
        weights = np.array([
            0.08, 0.07, 0.09, 0.06, 0.10, 0.08, 0.07, 0.06,
            0.09, 0.06, 0.05, 0.05, 0.04, 0.04, 0.03, 0.04, 0.03, 0.02
        ])
        weights = weights / weights.sum()
        
        # åŸºäºå“ˆå¸Œçš„ç¡®å®šæ€§éšæœºé€‰æ‹©
        hash_int = int(hashlib.md5(detector_upper.encode()).hexdigest(), 16) % 10000
        np.random.seed(hash_int)  # ç¡®ä¿åŒä¸€IDæ˜ å°„ç»“æœä¸€è‡´
        return np.random.choice(districts_list, p=weights)

    def fixed_deterministic_mapping(self, detector_id):
        """ä¿®å¤ç‰ˆç¡®å®šæ€§åŒºåŸŸæ˜ å°„ - è§£å†³æƒé‡é—®é¢˜"""
        hash_obj = hashlib.md5(detector_id.encode())
        hash_int = int(hash_obj.hexdigest(), 16)
        
        districts = [
            'Central and Western', 'Wan Chai', 'Eastern', 'Southern',
            'Yau Tsim Mong', 'Sham Shui Po', 'Kowloon City', 'Wong Tai Sin',
            'Kwun Tong', 'Kwai Tsing', 'Tsuen Wan', 'Tuen Mun', 
            'Yuen Long', 'North', 'Tai Po', 'Sha Tin', 'Sai Kung', 'Islands'
        ]
        
        weights = np.array([
            0.08, 0.07, 0.09, 0.06, 0.10, 0.08, 0.07, 0.06,
            0.09, 0.06, 0.05, 0.05, 0.04, 0.04, 0.03, 0.04, 0.03, 0.02
        ])
        
        weights = weights / weights.sum()
        
        np.random.seed(hash_int % 10000)
        return np.random.choice(districts, p=weights)

    def extract_hour_from_period(self, period_str):
        """ä»æ—¶é—´å­—ç¬¦ä¸²ä¸­æå–å°æ—¶"""
        try:
            if pd.isna(period_str):
                return 12
            if isinstance(period_str, str):
                time_part = period_str.split(' ')[-1]
                hours = time_part.split(':')[0]
                return int(hours) % 24
            return 12
        except:
            return 12

    def process_time_info_fixed(self, df):
        """ä¿®å¤ç‰ˆæ—¶é—´ä¿¡æ¯å¤„ç†"""
        df_time = df.copy()
        
        if 'period_from' in df_time.columns:
            try:
                df_time['hour'] = pd.to_datetime(df_time['period_from'], format='%H:%M:%S', errors='coerce').dt.hour
                if df_time['hour'].isna().any():
                    df_time['hour'] = pd.to_datetime(df_time['period_from'], format='%H:%M', errors='coerce').dt.hour
                
                df_time['hour'] = df_time['hour'].fillna(12).astype(np.int8)
                df_time['hour'] = df_time['hour'].clip(0, 23)
            except:
                df_time['hour'] = 12
        
        return df_time

    def validate_aggregated_traffic_data(self, aggregated_df):
        """éªŒè¯èšåˆåçš„äº¤é€šæ•°æ®"""
        logger.info("éªŒè¯èšåˆåçš„äº¤é€šæ•°æ®è´¨é‡...")
        
        # 1. æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['district', 'hour', 'volume_mean', 'speed_mean', 'record_count']
        missing_cols = [col for col in required_cols if col not in aggregated_df.columns]
        if missing_cols:
            logger.warning(f"èšåˆäº¤é€šæ•°æ®ç¼ºå¤±å¿…è¦åˆ—: {missing_cols}")
        
        # 2. æ£€æŸ¥åœ°åŒºå®Œæ•´æ€§ï¼ˆæ˜¯å¦è¦†ç›–æ‰€æœ‰é¦™æ¸¯è¡Œæ”¿åŒºï¼‰
        all_districts = set(self.hk_districts_info['district_english'])
        aggregated_districts = set(aggregated_df['district'].unique())
        missing_districts = all_districts - aggregated_districts
        if missing_districts:
            logger.warning(f"èšåˆäº¤é€šæ•°æ®ç¼ºå¤±åœ°åŒº: {missing_districts} (å…±{len(missing_districts)}ä¸ª)")
        else:
            logger.info(f"èšåˆäº¤é€šæ•°æ®è¦†ç›–æ‰€æœ‰{len(all_districts)}ä¸ªåœ°åŒº")
        
        # 3. æ£€æŸ¥å°æ—¶åˆ†å¸ƒï¼ˆæ˜¯å¦è¦†ç›–0-23å°æ—¶ï¼‰
        hours = set(aggregated_df['hour'].unique())
        missing_hours = set(range(24)) - hours
        if missing_hours:
            logger.warning(f"èšåˆäº¤é€šæ•°æ®ç¼ºå¤±å°æ—¶: {missing_hours}")
        else:
            logger.info("èšåˆäº¤é€šæ•°æ®è¦†ç›–æ‰€æœ‰24å°æ—¶")
        
        # 4. æ£€æŸ¥ç»Ÿè®¡å€¼åˆç†æ€§
        if 'volume_mean' in aggregated_df.columns:
            vol_outliers = aggregated_df[(aggregated_df['volume_mean'] < 0) | (aggregated_df['volume_mean'] > 10000)]
            if not vol_outliers.empty:
                logger.warning(f"äº¤é€šæµé‡å‡å€¼å¼‚å¸¸: {len(vol_outliers)}æ¡è®°å½•ï¼ˆèŒƒå›´åº”åœ¨0-10000ï¼‰")
        
        if 'speed_mean' in aggregated_df.columns:
            speed_outliers = aggregated_df[(aggregated_df['speed_mean'] < 0) | (aggregated_df['speed_mean'] > 120)]
            if not speed_outliers.empty:
                logger.warning(f"è½¦é€Ÿå‡å€¼å¼‚å¸¸: {len(speed_outliers)}æ¡è®°å½•ï¼ˆèŒƒå›´åº”åœ¨0-120ï¼‰")
        
        logger.info(f"èšåˆäº¤é€šæ•°æ®æ ¡éªŒå®Œæˆï¼Œå…±{len(aggregated_df)}æ¡è®°å½•")

    def validate_traffic_data(self, df):
        """éªŒè¯äº¤é€šæ•°æ®è´¨é‡"""
        logger.info("éªŒè¯äº¤é€šæ•°æ®è´¨é‡...")
        
        total_records = len(df)
        valid_detectors = df['detector_id'].nunique()
        valid_districts = df['district'].nunique()
        
        logger.info(f"  æ€»è®°å½•æ•°: {total_records:,}")
        logger.info(f"  å”¯ä¸€æ£€æµ‹å™¨æ•°: {valid_detectors:,}")
        logger.info(f"  è¦†ç›–åŒºåŸŸæ•°: {valid_districts}")
        
        completeness = {
            'volume': df['volume'].notna().mean() * 100,
            'speed': df['speed'].notna().mean() * 100,
            'hour': df['hour'].notna().mean() * 100,
            'district': (df['district'] != 'Unknown').mean() * 100
        }
        
        for field, percent in completeness.items():
            logger.info(f"  {field} å®Œæ•´æ€§: {percent:.1f}%")

    @memory_safe_operation("é¤å…æ•°æ®åŠ è½½")
    def load_restaurant_data_optimized(self):
        """ä¼˜åŒ–ç‰ˆé¤å…æ•°æ®åŠ è½½"""
        logger.info("ğŸª å¼€å§‹ä¼˜åŒ–ç‰ˆé¤å…æ•°æ®åŠ è½½...")
        
        restaurant_pattern = os.path.join(self.restaurant_data_path, "*.csv")
        restaurant_files = glob.glob(restaurant_pattern)
        
        if not restaurant_files:
            raise FileNotFoundError(f"åœ¨ {self.restaurant_data_path} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        
        logger.info(f"æ‰¾åˆ° {len(restaurant_files)} ä¸ªé¤å…æ•°æ®æ–‡ä»¶")
        
        all_restaurants = []
        
        for i, file_path in enumerate(restaurant_files):
            try:
                logger.info(f"å¤„ç†é¤å…æ–‡ä»¶ {i+1}/{len(restaurant_files)}: {os.path.basename(file_path)}")
                
                # ä½¿ç”¨åˆ†å—è¯»å–å¤„ç†å¤§æ–‡ä»¶
                chunk_list = []
                for chunk in pd.read_csv(file_path, 
                                    encoding='utf-8-sig',
                                    chunksize=50000,
                                    low_memory=False):
                    processed_chunk = self.process_restaurant_chunk(chunk)
                    if processed_chunk is not None and not processed_chunk.empty:
                        chunk_list.append(processed_chunk)
                    
                    # å†…å­˜ç®¡ç†
                    if len(chunk_list) % 5 == 0:
                        gc.collect()
                        memory_info = self.get_memory_usage()
                        if memory_info['percent'] > self.max_memory_threshold:
                            logger.warning("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæš‚åœå¤„ç†é¤å…æ•°æ®")
                            time.sleep(2)
                
                if chunk_list:
                    file_data = pd.concat(chunk_list, ignore_index=True)
                    all_restaurants.append(file_data)
                    logger.info(f"  âœ… æˆåŠŸåŠ è½½: {len(file_data):,} æ¡è®°å½•")
                
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        if all_restaurants:
            combined_data = pd.concat(all_restaurants, ignore_index=True)
            
            # ç²¾ç¡®å»é‡
            before_dedup = len(combined_data)
            combined_data = combined_data.drop_duplicates(
                subset=['licence_number', 'premises_name', 'address'], 
                keep='first'
            )
            after_dedup = len(combined_data)
            logger.info(f"ç²¾ç¡®å»é‡: {before_dedup:,} -> {after_dedup:,} æ¡è®°å½•")
            
            # å†…å­˜ä¼˜åŒ–
            combined_data = self.optimize_dataframe_memory(combined_data)
            
            # æ–°å¢ï¼šèšåˆåæ•°æ®æ ¡éªŒï¼ˆæŒ‰åœ°åŒºç»Ÿè®¡ï¼‰
            self.validate_aggregated_restaurant_data(combined_data)
            
            logger.info(f"âœ… é¤å…æ•°æ®åŠ è½½å®Œæˆ: {len(combined_data):,} æ¡è®°å½•")
            return combined_data
        else:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„é¤å…æ•°æ®")

    def process_restaurant_chunk(self, chunk):
        """å¤„ç†é¤å…æ•°æ®å—"""
        try:
            df_clean = chunk.copy()
            
            # åˆ—åæ ‡å‡†åŒ–
            column_mapping = {
                'district_name': 'district_name', 'district': 'district_name',
                'licence_type_description': 'licence_type', 'licence_type': 'licence_type',
                'premises_name': 'premises_name', 'address': 'address',
                'licence_number': 'licence_number', 'district_code': 'district_code'
            }
            
            for old_col, new_col in column_mapping.items():
                if old_col in df_clean.columns and new_col not in df_clean.columns:
                    df_clean[new_col] = df_clean[old_col]
            
            # å¤„ç†åœ°åŒºä¿¡æ¯
            df_clean = self.process_restaurant_district_fixed(df_clean)
            
            # æ•°æ®è¿‡æ»¤
            df_clean = df_clean[df_clean['premises_name'].notna()]
            df_clean = df_clean[df_clean['address'].notna()]
            
            return df_clean
            
        except Exception as e:
            logger.error(f"å¤„ç†é¤å…æ•°æ®å—æ—¶å‡ºé”™: {e}")
            return None

    def validate_aggregated_restaurant_data(self, restaurant_df):
        """éªŒè¯èšåˆåçš„é¤å…æ•°æ®ï¼ˆæŒ‰åœ°åŒºç»Ÿè®¡ï¼‰"""
        logger.info("éªŒè¯èšåˆåçš„é¤å…æ•°æ®è´¨é‡...")
        
        # 1. æŒ‰åœ°åŒºèšåˆç»Ÿè®¡
        restaurant_agg = restaurant_df.groupby('district_english').size().reset_index(name='restaurant_count')
        
        # 2. æ£€æŸ¥åœ°åŒºè¦†ç›–å®Œæ•´æ€§
        all_districts = set(self.hk_districts_info['district_english'])
        restaurant_districts = set(restaurant_agg['district_english'].unique())
        missing_districts = all_districts - restaurant_districts
        if missing_districts:
            logger.warning(f"é¤å…æ•°æ®ç¼ºå¤±åœ°åŒº: {missing_districts} (å…±{len(missing_districts)}ä¸ª)")
        else:
            logger.info(f"é¤å…æ•°æ®è¦†ç›–æ‰€æœ‰{len(all_districts)}ä¸ªåœ°åŒº")
        
        # 3. æ£€æŸ¥å¼‚å¸¸å€¼ï¼ˆåœ°åŒºé¤å…æ•°é‡æ˜¯å¦ä¸º0æˆ–å¼‚å¸¸é«˜ï¼‰
        zero_restaurant = restaurant_agg[restaurant_agg['restaurant_count'] == 0]
        if not zero_restaurant.empty:
            logger.warning(f"ä»¥ä¸‹åœ°åŒºé¤å…æ•°é‡ä¸º0: {zero_restaurant['district_english'].tolist()}")
        
        # 4. æ£€æŸ¥ä¸äººå£å¯†åº¦çš„å…³è”æ€§ï¼ˆåˆç†æ€§æ ¡éªŒï¼‰
        # åˆå¹¶åœ°åŒºäººå£æ•°æ®
        district_pop = self.hk_districts_info[['district_english', 'population_density']]
        merged = pd.merge(restaurant_agg, district_pop, on='district_english', how='left')
        # è®¡ç®—æ¯ä¸‡äººé¤å…æ•°é‡ï¼ˆç®€å•åˆç†æ€§æ ¡éªŒï¼‰
        merged['rest_per_10k_people'] = merged['restaurant_count'] / (merged['population_density'] * 10)  # è¿‘ä¼¼è®¡ç®—
        abnormal_ratio = merged[(merged['rest_per_10k_people'] < 0.1) | (merged['rest_per_10k_people'] > 50)]
        if not abnormal_ratio.empty:
            logger.warning(f"é¤å…å¯†åº¦å¼‚å¸¸åœ°åŒº: {abnormal_ratio['district_english'].tolist()}")
        
        logger.info(f"èšåˆé¤å…æ•°æ®æ ¡éªŒå®Œæˆï¼Œå…±è¦†ç›–{len(restaurant_districts)}ä¸ªåœ°åŒº")

    def process_restaurant_district_fixed(self, df):
        """ä¿®å¤ç‰ˆé¤å…åœ°åŒºä¿¡æ¯å¤„ç†"""
        df_district = df.copy()
        
        if 'district_name' in df_district.columns:
            df_district['district_english'] = df_district['district_name'].map(self.district_mapping)
        
        elif 'district_code' in df_district.columns:
            district_code_mapping = {
                '1': 'ä¸­è¥¿å€', '2': 'åŒ—å€', '3': 'å¤§åŸ”å€', '4': 'ç£ä»”å€',
                '5': 'æ²¹å°–æ—ºå€', '6': 'æ·±æ°´åŸ—å€', '7': 'ä¹é¾åŸå€', '8': 'é»ƒå¤§ä»™å€',
                '9': 'è§€å¡˜å€', '10': 'è‘µé’å€', '11': 'æ±å€', '12': 'èƒç£å€',
                '13': 'å±¯é–€å€', '14': 'å…ƒæœ—å€', '15': 'å—å€', '16': 'æ²™ç”°å€',
                '17': 'è¥¿è²¢å€', '18': 'é›¢å³¶å€'
            }
            df_district['district_name'] = df_district['district_code'].map(district_code_mapping)
            df_district['district_english'] = df_district['district_name'].map(self.district_mapping)
        
        elif 'address' in df_district.columns:
            df_district['district_name'] = df_district['address'].apply(self.extract_district_from_address_fixed)
            df_district['district_english'] = df_district['district_name'].map(self.district_mapping)
        
        else:
            df_district['district_english'] = 'Unknown'
        
        df_district['district_english'] = df_district['district_english'].fillna('Unknown')
        
        return df_district

    def extract_district_from_address_fixed(self, address):
        """å‘é‡åŒ–ä¼˜åŒ–çš„åœ°å€æå–åœ°åŒº"""
        if not isinstance(address, pd.Series):
            address = pd.Series(address)
        
        address_str = address.fillna('').astype(str).str.upper()
        district = pd.Series(['Unknown'] * len(address_str), index=address_str.index)
        
        district_keywords = {
            # ä¸­è¥¿åŒº
            'CENTRAL': 'ä¸­è¥¿å€', 'WESTERN': 'ä¸­è¥¿å€', 'SHEUNG WAN': 'ä¸­è¥¿å€',
            'MID-LEVELS': 'ä¸­è¥¿å€', 'THE PEAK': 'ä¸­è¥¿å€', 'KENNEDY TOWN': 'ä¸­è¥¿å€',
            # æ¹¾ä»”åŒº
            'WAN CHAI': 'ç£ä»”å€', 'CAUSEWAY BAY': 'ç£ä»”å€', 'HAPPY VALLEY': 'ç£ä»”å€',
            # ä¸œåŒº
            'EASTERN': 'æ±å€', 'NORTH POINT': 'æ±å€', 'QUARRY BAY': 'æ±å€', 'TAIKOO': 'æ±å€',
            'SAI WAN HO': 'æ±å€', 'SHAU KEI WAN': 'æ±å€', 'CHAI WAN': 'æ±å€',
            # å—åŒº
            'SOUTHERN': 'å—å€', 'ABERDEEN': 'å—å€', 'REPULSE BAY': 'å—å€', 'DEEP WATER BAY': 'å—å€',
            'STANLEY': 'å—å€', 'WONG CHUK HANG': 'å—å€', 'AP LEI CHAU': 'å—å€',
            # æ²¹å°–æ—ºåŒº
            'YAU MA TEI': 'æ²¹å°–æ—ºå€', 'TSIM SHA TSUI': 'æ²¹å°–æ—ºå€', 'MONG KOK': 'æ²¹å°–æ—ºå€',
            'JORDAN': 'æ²¹å°–æ—ºå€', 'TAI KOK TSUI': 'æ²¹å°–æ—ºå€',
            # æ·±æ°´åŸ—åŒº
            'SHAM SHUI PO': 'æ·±æ°´åŸ—å€', 'CHEUNG SHA WAN': 'æ·±æ°´åŸ—å€', 'MEI FO': 'æ·±æ°´åŸ—å€',
            # ä¹é¾™åŸåŒº
            'KOWLOON CITY': 'ä¹é¾åŸå€', 'HOMANTIN': 'ä¹é¾åŸå€', 'KOWLOON TONG': 'ä¹é¾åŸå€',
            'TO KWA WAN': 'ä¹é¾åŸå€', 'HUNG HOM': 'ä¹é¾åŸå€',
            # é»„å¤§ä»™åŒº
            'WONG TAI SIN': 'é»ƒå¤§ä»™å€', 'DIAMOND HILL': 'é»ƒå¤§ä»™å€', 'WANG TAU HOM': 'é»ƒå¤§ä»™å€',
            # è§‚å¡˜åŒº
            'KWUN TONG': 'è§€å¡˜å€', 'YAU TONG': 'è§€å¡˜å€', 'LEI YUE MUN': 'è§€å¡˜å€',
            'NGAU TAU KOK': 'è§€å¡˜å€', 'LAM TIN': 'è§€å¡˜å€',
            # è‘µé’åŒº
            'KWAI CHUNG': 'è‘µé’å€', 'TSING YI': 'è‘µé’å€', 'KWAI FONG': 'è‘µé’å€',
            # èƒæ¹¾åŒº
            'TSUEN WAN': 'èƒç£å€', 'TSUEN WAN WEST': 'èƒç£å€',
            # å±¯é—¨åŒº
            'TUEN MUN': 'å±¯é–€å€', 'CASTLE PEAK': 'å±¯é–€å€',
            # å…ƒæœ—åŒº
            'YUEN LONG': 'å…ƒæœ—å€', 'TIN SHUI WAI': 'å…ƒæœ—å€', 'YUEN LONG TOWN': 'å…ƒæœ—å€',
            # åŒ—åŒº
            'NORTH': 'åŒ—å€', 'SHEUNG SHUI': 'åŒ—å€', 'FANLING': 'åŒ—å€', 'LUK KENG': 'åŒ—å€',
            # å¤§åŸ”åŒº
            'TAI PO': 'å¤§åŸ”å€', 'TAI PO MARKET': 'å¤§åŸ”å€', 'TAI PO KAU': 'å¤§åŸ”å€',
            # æ²™ç”°åŒº
            'SHA TIN': 'æ²™ç”°å€', 'MA ON SHAN': 'æ²™ç”°å€', 'FO TAN': 'æ²™ç”°å€',
            # è¥¿è´¡åŒº
            'SAI KUNG': 'è¥¿è²¢å€', 'CLEAR WATER BAY': 'è¥¿è²¢å€', 'PAK TAM CHUNG': 'è¥¿è²¢å€',
            # ç¦»å²›åŒº
            'ISLANDS': 'é›¢å³¶å€', 'LANTAU': 'é›¢å³¶å€', 'CHEUNG CHAU': 'é›¢å³¶å€', 'LAMMA': 'é›¢å³¶å€',
            'DISCOVERY BAY': 'é›¢å³¶å€', 'TUNG CHUNG': 'é›¢å³¶å€'
        }
        
        for pattern, dist in district_keywords.items():
            mask = address_str.str.contains(pattern, regex=True)
            district[mask] = dist
        
        return district.values if len(district) > 1 else district.iloc[0]

    def validate_aggregated_features(self, feature_df):
        """éªŒè¯ç‰¹å¾å·¥ç¨‹åçš„èšåˆç‰¹å¾æ•°æ®"""
        logger.info("éªŒè¯èšåˆåçš„ç‰¹å¾æ•°æ®è´¨é‡...")
        
        # 1. æ£€æŸ¥å…³é”®ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
        required_features = ['district', 'hour', 'volume_mean', 'speed_mean', 'restaurant_count']
        missing_features = [f for f in required_features if f not in feature_df.columns]
        if missing_features:
            logger.warning(f"ç‰¹å¾æ•°æ®ç¼ºå¤±å¿…è¦ç‰¹å¾: {missing_features}")
        
        # 2. æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
        missing_ratio = feature_df.isnull().mean().sort_values(ascending=False)
        high_missing = missing_ratio[missing_ratio > 0.2]  # ç¼ºå¤±ç‡>20%çš„ç‰¹å¾
        if not high_missing.empty:
            logger.warning(f"é«˜ç¼ºå¤±ç‡ç‰¹å¾: {high_missing.to_dict()}")
        
        # 3. æ£€æŸ¥ç‰¹å¾ç›¸å…³æ€§ï¼ˆé¿å…æ˜æ˜¾ä¸åˆç†çš„å…³è”ï¼‰
        numeric_features = feature_df.select_dtypes(include=[np.number]).columns
        if len(numeric_features) >= 2:
            corr = feature_df[numeric_features].corr().abs()
            high_corr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).stack().sort_values(ascending=False)
            high_corr = high_corr[high_corr > 0.9]  # å¼ºç›¸å…³ç‰¹å¾ï¼ˆ>0.9ï¼‰
            if not high_corr.empty:
                logger.warning(f"å¼ºç›¸å…³ç‰¹å¾å¯¹: {[(i[0], i[1]) for i in high_corr.index]}")
        
        logger.info(f"ç‰¹å¾æ•°æ®æ ¡éªŒå®Œæˆï¼Œå…±{len(feature_df)}æ¡è®°å½•ï¼Œ{len(feature_df.columns)}ä¸ªç‰¹å¾")

    @memory_safe_operation("é«˜çº§ç‰¹å¾å·¥ç¨‹")
    def advanced_feature_engineering(self, traffic_data, restaurant_data):
        """å¤æ‚ç‰¹å¾å·¥ç¨‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("å¼€å§‹é«˜çº§ç‰¹å¾å·¥ç¨‹...")
        
        try:
            # 1. æ•°æ®æ¸…æ´—ï¼ˆé’ˆå¯¹èšåˆç‰¹å¾ï¼‰
            traffic_clean = traffic_data.copy()
            
            # å¤„ç†å¼‚å¸¸å€¼ï¼ˆåŸºäº3ÏƒåŸåˆ™ï¼‰
            for col in ['volume_mean', 'speed_mean']:
                mean = traffic_clean[col].mean()
                std = traffic_clean[col].std()
                lower_bound = mean - 3 * std
                upper_bound = mean + 3 * std
                traffic_clean = traffic_clean[(traffic_clean[col] >= lower_bound) & 
                                            (traffic_clean[col] <= upper_bound)]
                logger.info(f"æ¸…æ´— {col} å¼‚å¸¸å€¼: ä¿ç•™ {len(traffic_clean)}/{len(traffic_data)} æ¡è®°å½•")
            
            # å¤„ç†0å€¼ï¼ˆç”¨åŒºåŸŸå‡å€¼å¡«å……ï¼‰
            for col in ['volume_mean', 'speed_mean']:
                district_means = traffic_clean.groupby('district')[col].transform('mean')
                traffic_clean[col] = traffic_clean[col].replace(0, np.nan).fillna(district_means)
            
            # 2. è¡ç”Ÿç‰¹å¾è®¡ç®—
            # é€Ÿåº¦-æµé‡æ¯”ï¼ˆäº¤é€šæ•ˆç‡æŒ‡æ ‡ï¼‰
            traffic_clean['speed_volume_ratio'] = traffic_clean['speed_mean'] / (traffic_clean['volume_mean'] + 1e-6)  # é¿å…é™¤é›¶
            
            # å°æ—¶çº§æ³¢åŠ¨ç‡ï¼ˆåŒä¸€åŒºåŸŸä¸åŒæ—¶æ®µçš„æµé‡æ³¢åŠ¨ï¼‰
            traffic_clean['volume_volatility'] = traffic_clean.groupby('district')['volume_mean'].transform(
                lambda x: x / x.mean() - 1  # ç›¸å¯¹å‡å€¼çš„æ³¢åŠ¨æ¯”ä¾‹
            )
            
            # æ—¶é—´äº¤äº’ç‰¹å¾
            traffic_clean['hour_volume_product'] = traffic_clean['hour'] * traffic_clean['volume_mean']  # æ—¶æ®µæµé‡å¼ºåº¦
            traffic_clean['is_peak_hour'] = ((traffic_clean['hour'] >= 7) & (traffic_clean['hour'] <= 9)) | \
                                        ((traffic_clean['hour'] >= 17) & (traffic_clean['hour'] <= 19)).astype(np.int8)
            
            # 3. é¤å…æ•°æ®èšåˆï¼ˆæŒ‰åŒºåŸŸç»Ÿè®¡ï¼‰
            restaurant_agg = restaurant_data.groupby('district_english').agg({
                'licence_number': 'count',  # é¤å…æ•°é‡
                'premises_name': lambda x: x.nunique()  # ç‹¬ç‰¹é¤å…åç§°æ•°ï¼ˆå»é‡ï¼‰
            }).rename(columns={
                'licence_number': 'restaurant_count',
                'premises_name': 'unique_restaurant_count'
            }).reset_index()
            
            # 4. æ•°æ®åˆå¹¶ï¼ˆå†…è¿æ¥å‡å°‘æ— æ•ˆè¡Œï¼‰
            merged_data = pd.merge(
                traffic_clean, 
                restaurant_agg, 
                left_on='district', 
                right_on='district_english',
                how='inner'
            ).drop(columns=['district_english'])
            
            # 5. ç¦»æ•£åŒ–è¿ç»­ç‰¹å¾ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
            merged_data['volume_bin'] = pd.cut(
                merged_data['volume_mean'], 
                bins=5, 
                labels=['very_low', 'low', 'medium', 'high', 'very_high']
            ).astype('category')
            
            merged_data['speed_bin'] = pd.qcut(
                merged_data['speed_mean'], 
                q=4, 
                labels=['slow', 'medium_slow', 'medium_fast', 'fast']
            ).astype('category')
            
            # 6. åŒºåŸŸç‰¹å¾å…³è”ï¼ˆåŠ å…¥é¦™æ¸¯è¡Œæ”¿åŒºå±æ€§æ•°æ®ï¼‰
            final_data = pd.merge(
                merged_data,
                self.hk_districts_info,
                left_on='district',
                right_on='district_english',
                how='left'
            ).drop(columns=['district_english'])
            
            # ç¡®ä¿districtä¸ºcategoryç±»å‹
            final_data['district'] = final_data['district'].astype('category')
            self.validate_aggregated_features(feature_df)
            logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: ç”Ÿæˆ {final_data.shape[1]} ä¸ªç‰¹å¾ï¼Œ{len(final_data)} æ¡è®°å½•")
            return self.optimize_dataframe_memory(final_data)
            
        except Exception as e:
            logger.error(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def create_traffic_features(self, traffic_data):
        """åˆ›å»ºäº¤é€šç‰¹å¾"""
        # æ£€æŸ¥äº¤é€šæ•°æ®ä¸­çš„åˆ—åï¼Œä½¿ç”¨æ­£ç¡®çš„èšåˆåˆ—å
        traffic_stats = traffic_data.groupby(['district', 'hour']).agg({
            # å°†'speed'æ”¹ä¸º'speed_mean'ï¼Œ'volume'æ”¹ä¸º'volume_mean'
            'speed_mean': ['mean', 'std', 'max', 'min'],
            'volume_mean': ['mean', 'std', 'max', 'min', 'sum']
        }).reset_index()
        
        # é‡å‘½ååˆ—
        traffic_stats.columns = [
            'district', 'hour',
            'speed_mean', 'speed_std', 'speed_max', 'speed_min',
            'volume_mean', 'volume_std', 'volume_max', 'volume_min', 'volume_total'
        ]
        
        # æ·»åŠ äº¤é€šæ‹¥å µæŒ‡æ•° (é€Ÿåº¦ä½ä¸”æµé‡é«˜è¡¨ç¤ºæ‹¥å µ)
        traffic_stats['congestion_index'] = (
            (traffic_stats['volume_mean'] / traffic_stats['volume_mean'].max()) * 0.5 +
            (1 - traffic_stats['speed_mean'] / traffic_stats['speed_mean'].max()) * 0.5
        )
        
        # æ·»åŠ å°æ—¶æ—¶æ®µç‰¹å¾
        traffic_stats['is_peak_hour'] = traffic_stats['hour'].apply(
            lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0
        )
        traffic_stats['time_period'] = pd.cut(
            traffic_stats['hour'],
            bins=[0, 6, 10, 15, 20, 24],
            labels=['early_morning', 'morning', 'midday', 'evening', 'night']
        )
        
        return traffic_stats

    def create_poi_features(self, restaurant_data):
        """åˆ›å»ºå¤æ‚çš„POIç‰¹å¾"""
        logger.info("  åˆ›å»ºPOIç‰¹å¾...")
        
        # åŸºç¡€POIç»Ÿè®¡
        poi_basic = restaurant_data.groupby('district_english').agg({
            'premises_name': 'count',
            'licence_number': 'nunique',
            'licence_type': lambda x: x.nunique()
        }).reset_index()
        poi_basic.columns = ['district', 'poi_count', 'unique_licences', 'licence_types']
        
        # POIå¤šæ ·æ€§ç‰¹å¾
        diversity_features = self.calculate_poi_diversity(restaurant_data)
        
        # ç©ºé—´åˆ†å¸ƒç‰¹å¾
        spatial_features = self.calculate_spatial_distribution(restaurant_data)
        
        # å•†ä¸šé›†èšç‰¹å¾
        commercial_features = self.calculate_commercial_agglomeration(restaurant_data)
        
        # åˆå¹¶POIç‰¹å¾
        poi_features = poi_basic.merge(diversity_features, on='district', how='left')
        poi_features = poi_features.merge(spatial_features, on='district', how='left')
        poi_features = poi_features.merge(commercial_features, on='district', how='left')
        
        # è®¡ç®—å¯†åº¦æŒ‡æ ‡
        district_info = self.hk_districts_info[['district_english', 'area_sqkm', 'population_2023']]
        district_info.columns = ['district', 'area_sqkm', 'population']
        
        poi_features = poi_features.merge(district_info, on='district', how='left')
        poi_features['poi_density'] = poi_features['poi_count'] / poi_features['area_sqkm']
        poi_features['poi_per_capita'] = poi_features['poi_count'] / poi_features['population']
        
        return self.optimize_dataframe_memory(poi_features)

    def create_spatiotemporal_features(self, traffic_data, restaurant_data):
        """åˆ›å»ºæ—¶ç©ºç‰¹å¾"""
        logger.info("  åˆ›å»ºæ—¶ç©ºç‰¹å¾...")
        
        # æ—¶é—´åºåˆ—ç‰¹å¾
        temporal_features = self.calculate_temporal_patterns(traffic_data)
        
        # ç©ºé—´ç›¸å…³æ€§ç‰¹å¾
        spatial_corr_features = self.calculate_spatial_correlation(traffic_data, restaurant_data)
        
        # æ—¶ç©ºèšç±»ç‰¹å¾
        clustering_features = self.calculate_spatiotemporal_clustering(traffic_data, restaurant_data)
        
        # åˆå¹¶æ—¶ç©ºç‰¹å¾
        spatiotemporal_features = temporal_features.merge(spatial_corr_features, on='district', how='left')
        spatiotemporal_features = spatiotemporal_features.merge(clustering_features, on='district', how='left')
        
        return self.optimize_dataframe_memory(spatiotemporal_features)

    def create_economic_features(self):
        """åˆ›å»ºç»æµç‰¹å¾"""
        logger.info("  åˆ›å»ºç»æµç‰¹å¾...")
        economic_features = self.hk_districts_info[['district_english', 'gdp_per_capita_hkd', 
                                     'commercial_importance', 'economic_index']].rename(
                                         columns={'district_english': 'district'})
        return self.optimize_dataframe_memory(economic_features)

    def calculate_time_variability(self, volume_series):
        """è®¡ç®—æ—¶é—´å˜å¼‚æ€§"""
        if len(volume_series) < 2:
            return 0
        return np.std(volume_series) / np.mean(volume_series)

    def calculate_congestion_index(self, speed_series):
        """è®¡ç®—æ‹¥å µæŒ‡æ•°"""
        if len(speed_series) < 2:
            return 0
        return (1 - (np.mean(speed_series) / 80)) * 100

    def calculate_peak_hour_features(self, traffic_data):
        """è®¡ç®—é«˜å³°æ—¶æ®µç‰¹å¾"""
        peak_features = []
        
        for district in traffic_data['district'].unique():
            district_data = traffic_data[traffic_data['district'] == district]
            
            hourly_volume = district_data.groupby('hour')['volume'].mean()
            
            if len(hourly_volume) > 0:
                morning_peak = hourly_volume.loc[hourly_volume.index.isin([7,8,9])].mean() if any(hourly_volume.index.isin([7,8,9])) else 0
                evening_peak = hourly_volume.loc[hourly_volume.index.isin([17,18,19])].mean() if any(hourly_volume.index.isin([17,18,19])) else 0
                off_peak = hourly_volume.loc[~hourly_volume.index.isin([7,8,9,17,18,19])].mean() if len(hourly_volume.loc[~hourly_volume.index.isin([7,8,9,17,18,19])]) > 0 else 0
                
                peak_ratio = (morning_peak + evening_peak) / (2 * off_peak) if off_peak > 0 else 0
                
                peak_features.append({
                    'district': district,
                    'morning_peak_volume': morning_peak,
                    'evening_peak_volume': evening_peak,
                    'off_peak_volume': off_peak,
                    'peak_ratio': peak_ratio
                })
        
        return pd.DataFrame(peak_features)

    def calculate_poi_diversity(self, restaurant_data):
        """è®¡ç®—POIå¤šæ ·æ€§"""
        diversity_data = []
        
        for district in restaurant_data['district_english'].unique():
            district_poi = restaurant_data[restaurant_data['district_english'] == district]
            
            licence_diversity = district_poi['licence_type'].nunique()
            business_richness = len(district_poi)
            
            licence_counts = district_poi['licence_type'].value_counts()
            total = licence_counts.sum()
            shannon_diversity = -sum((count/total) * np.log(count/total) for count in licence_counts if count > 0) if total > 0 else 0
            
            diversity_data.append({
                'district': district,
                'licence_diversity': licence_diversity,
                'business_richness': business_richness,
                'shannon_diversity': shannon_diversity
            })
        
        return pd.DataFrame(diversity_data)

    def calculate_spatial_distribution(self, restaurant_data):
        """è®¡ç®—ç©ºé—´åˆ†å¸ƒç‰¹å¾"""
        spatial_data = []
        
        for district in restaurant_data['district_english'].unique():
            district_poi = restaurant_data[restaurant_data['district_english'] == district]
            
            poi_count = len(district_poi)
            spatial_clustering = min(poi_count / 100, 1.0)
            
            commercial_concentration = poi_count / self.hk_districts_info[
                self.hk_districts_info['district_english'] == district
            ]['area_sqkm'].values[0] if poi_count > 0 else 0
            
            spatial_data.append({
                'district': district,
                'spatial_clustering': spatial_clustering,
                'commercial_concentration': commercial_concentration
            })
        
        return pd.DataFrame(spatial_data)

    def calculate_commercial_agglomeration(self, restaurant_data):
        """è®¡ç®—å•†ä¸šé›†èšç‰¹å¾"""
        commercial_data = []
        
        for district in restaurant_data['district_english'].unique():
            district_poi = restaurant_data[restaurant_data['district_english'] == district]
            poi_count = len(district_poi)
            
            district_info = self.hk_districts_info[
                self.hk_districts_info['district_english'] == district
            ]
            
            if len(district_info) > 0:
                area = district_info['area_sqkm'].values[0]
                population = district_info['population_2023'].values[0]
                
                agglomeration_index = (poi_count / area) * (poi_count / population) * 1000
                
                commercial_data.append({
                    'district': district,
                    'commercial_agglomeration': agglomeration_index
                })
        
        return pd.DataFrame(commercial_data)

    def calculate_temporal_patterns(self, traffic_data):
        """è®¡ç®—æ—¶é—´æ¨¡å¼ç‰¹å¾"""
        temporal_data = []
        
        for district in traffic_data['district'].unique():
            district_traffic = traffic_data[traffic_data['district'] == district]
            
            if len(district_traffic) > 0:
                daily_variability = district_traffic.groupby('hour')['volume'].mean().std()
                hourly_trend = self.calculate_hourly_trend(district_traffic)
                
                temporal_data.append({
                    'district': district,
                    'daily_variability': daily_variability,
                    'hourly_trend_strength': hourly_trend
                })
        
        return pd.DataFrame(temporal_data)

    def calculate_hourly_trend(self, district_traffic):
        """è®¡ç®—å°æ—¶è¶‹åŠ¿å¼ºåº¦"""
        hourly_avg = district_traffic.groupby('hour')['volume'].mean()
        if len(hourly_avg) > 1:
            hours = np.array(hourly_avg.index).reshape(-1, 1)
            volumes = hourly_avg.values
            slope = np.polyfit(hours.flatten(), volumes, 1)[0]
            return abs(slope)
        return 0

    def calculate_spatial_correlation(self, traffic_data, restaurant_data):
        """è®¡ç®—ç©ºé—´ç›¸å…³æ€§ç‰¹å¾"""
        spatial_corr_data = []
        
        traffic_by_district = traffic_data.groupby('district').agg({
            'volume': 'mean',
            'speed': 'mean'
        }).reset_index()
        
        restaurant_by_district = restaurant_data.groupby('district_english').agg({
            'premises_name': 'count'
        }).reset_index().rename(columns={'district_english': 'district'})
        
        merged_data = traffic_by_district.merge(restaurant_by_district, on='district', how='inner')
        
        if len(merged_data) > 1:
            volume_poi_corr = merged_data['volume'].corr(merged_data['premises_name'])
            speed_poi_corr = merged_data['speed'].corr(merged_data['premises_name'])
            
            for district in merged_data['district']:
                spatial_corr_data.append({
                    'district': district,
                    'volume_poi_correlation': volume_poi_corr,
                    'speed_poi_correlation': speed_poi_corr
                })
        else:
            for district in traffic_data['district'].unique():
                spatial_corr_data.append({
                    'district': district,
                    'volume_poi_correlation': 0,
                    'speed_poi_correlation': 0
                })
        
        return pd.DataFrame(spatial_corr_data)

    def calculate_spatiotemporal_clustering(self, traffic_data, restaurant_data):
        """è®¡ç®—æ—¶ç©ºèšç±»ç‰¹å¾"""
        clustering_data = []
        
        for district in traffic_data['district'].unique():
            district_traffic = traffic_data[traffic_data['district'] == district]
            district_restaurants = restaurant_data[restaurant_data['district_english'] == district]
            
            traffic_cluster_score = self.analyze_traffic_patterns(district_traffic)
            poi_cluster_score = self.analyze_poi_distribution(district_restaurants)
            
            clustering_data.append({
                'district': district,
                'traffic_pattern_score': traffic_cluster_score,
                'poi_distribution_score': poi_cluster_score
            })
        
        return pd.DataFrame(clustering_data)

    def analyze_traffic_patterns(self, district_traffic):
        """åˆ†æäº¤é€šæ¨¡å¼"""
        if len(district_traffic) < 10:
            return 0.5
        
        hourly_pattern = district_traffic.groupby('hour')['volume'].mean().values
        
        if len(hourly_pattern) > 1:
            complexity = np.std(hourly_pattern) / np.mean(hourly_pattern)
            return min(complexity, 1.0)
        
        return 0.5

    def analyze_poi_distribution(self, district_restaurants):
        """åˆ†æPOIåˆ†å¸ƒ"""
        poi_count = len(district_restaurants)
        
        if poi_count == 0:
            return 0
        
        licence_distribution = district_restaurants['licence_type'].value_counts()
        diversity = len(licence_distribution) / poi_count
        
        return diversity

    def merge_all_features(self, traffic_features, poi_features, spatiotemporal_features, economic_features):
        """åˆå¹¶æ‰€æœ‰ç‰¹å¾"""
        logger.info("  åˆå¹¶æ‰€æœ‰ç‰¹å¾...")
        
        merged_data = traffic_features.merge(poi_features, on='district', how='left')
        merged_data = merged_data.merge(spatiotemporal_features, on='district', how='left')
        merged_data = merged_data.merge(economic_features, on='district', how='left')
        
        merged_data = merged_data.fillna(0)
        merged_data = self.optimize_dataframe_memory(merged_data)
        
        logger.info(f"  æœ€ç»ˆç‰¹å¾ç»´åº¦: {merged_data.shape}")
        
        return merged_data

    def calculate_advanced_vitality_index(self, features_data):
        """è®¡ç®—é«˜çº§æ´»åŠ›æŒ‡æ•°"""
        key_metrics = ['volume_mean', 'poi_density', 'commercial_agglomeration', 
                      'economic_index', 'peak_ratio']
        
        available_metrics = [metric for metric in key_metrics if metric in features_data.columns]
        
        if len(available_metrics) == 0:
            features_data['vitality_index'] = 50
            return features_data
        
        scaler = StandardScaler()
        normalized_metrics = scaler.fit_transform(features_data[available_metrics])
        
        weights = np.array([0.3, 0.25, 0.2, 0.15, 0.1])[:len(available_metrics)]
        weights = weights / weights.sum()
        
        vitality_index = np.dot(normalized_metrics, weights)
        
        min_val = vitality_index.min()
        max_val = vitality_index.max()
        
        if max_val > min_val:
            vitality_index = ((vitality_index - min_val) / (max_val - min_val)) * 100
        else:
            vitality_index = np.ones_like(vitality_index) * 50
        
        features_data['vitality_index'] = vitality_index
        
        return features_data

    @memory_safe_operation("æœºå™¨å­¦ä¹ åˆ†æ")
    def advanced_ml_analysis(self, features_data):
        """é«˜çº§æœºå™¨å­¦ä¹ åˆ†æ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("å¼€å§‹é«˜çº§æœºå™¨å­¦ä¹ åˆ†æ...")
        
        X, y, feature_names = self.prepare_ml_data(features_data)
        
        if X.shape[0] == 0:
            logger.warning("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ")
            return {}
        
        # ç‰¹å¾é€‰æ‹©
        selected_features = self.feature_selection_analysis(X, y, feature_names)
        
        # å¤šæ¨¡å‹æ¯”è¾ƒ
        model_results = self.compare_ml_models(X, y)
        
        # èšç±»åˆ†æ
        clustering_results = self.perform_clustering_analysis(X, features_data)
        
        # å¼‚å¸¸æ£€æµ‹
        anomaly_results = self.perform_anomaly_detection(X, features_data)
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        importance_analysis = self.analyze_feature_importance(X, y, feature_names)
        
        results = {
            'feature_selection': selected_features,
            'model_comparison': model_results,
            'clustering_analysis': clustering_results,
            'anomaly_detection': anomaly_results,
            'feature_importance': importance_analysis,
            'feature_matrix_shape': X.shape,
            'target_variable_stats': {'mean': y.mean(), 'std': y.std()} if len(y) > 0 else {}
        }
        
        logger.info("âœ… é«˜çº§æœºå™¨å­¦ä¹ åˆ†æå®Œæˆ")
        return results

    def prepare_ml_data(self, features_data):
        """å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®"""
        numeric_features = features_data.select_dtypes(include=[np.number]).columns.tolist()
        
        exclude_cols = ['district', 'hour', 'vitality_index']
        feature_cols = [col for col in numeric_features if col not in exclude_cols]
        
        if len(feature_cols) == 0:
            return np.array([]), np.array([]), []
        
        X = features_data[feature_cols].values
        feature_names = feature_cols
        
        y = features_data['vitality_index'].values if 'vitality_index' in features_data.columns else np.ones(len(features_data)) * 50
        
        return X, y, feature_names

    def feature_selection_analysis(self, X, y, feature_names):
        """ç‰¹å¾é€‰æ‹©åˆ†æ"""
        logger.info("  è¿›è¡Œç‰¹å¾é€‰æ‹©åˆ†æ...")
        
        if X.shape[1] == 0:
            return {}
        
        results = {}
        
        # åŸºäºç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©
        correlations = []
        for i, feature in enumerate(feature_names):
            if len(y) > 1 and not np.isnan(X[:, i]).all():
                corr = np.corrcoef(X[:, i], y)[0, 1] if not np.isnan(X[:, i]).all() else 0
                correlations.append((feature, abs(corr)))
        
        correlations.sort(key=lambda x: x[1], reverse=True)
        results['correlation_ranking'] = correlations[:10]
        
        return results

    def compare_ml_models(self, X, y):
        """æ¯”è¾ƒå¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹"""
        logger.info("  æ¯”è¾ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        if X.shape[0] < 10:
            return {}
        
        models = {
            'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42),  # å‡å°‘æ ‘çš„æ•°é‡
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, random_state=42),
            'Ridge Regression': Ridge(alpha=1.0),
        }
        
        results = {}
        
        for name, model in models.items():
            try:
                tscv = TimeSeriesSplit(n_splits=min(3, X.shape[0] // 2))  # å‡å°‘äº¤å‰éªŒè¯æŠ˜æ•°
                cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')
                
                model.fit(X, y)
                y_pred = model.predict(X)
                
                results[name] = {
                    'cv_r2_mean': cv_scores.mean(),
                    'cv_r2_std': cv_scores.std(),
                    'train_r2': r2_score(y, y_pred),
                    'train_rmse': np.sqrt(mean_squared_error(y, y_pred)),
                }
            except Exception as e:
                logger.warning(f"æ¨¡å‹ {name} è®­ç»ƒå¤±è´¥: {e}")
                results[name] = {'error': str(e)}
        
        return results

    def perform_clustering_analysis(self, X, features_data):
        """æ‰§è¡Œèšç±»åˆ†æ"""
        logger.info("  è¿›è¡Œèšç±»åˆ†æ...")
        
        if X.shape[0] < 3:
            return {}
        
        results = {}
        
        try:
            kmeans = KMeans(n_clusters=min(3, X.shape[0]), random_state=42)  # å‡å°‘èšç±»æ•°
            cluster_labels = kmeans.fit_predict(X)
            
            features_data = features_data.copy()
            features_data['cluster'] = cluster_labels
            
            cluster_profiles = features_data.groupby('cluster').mean()
            
            results['kmeans'] = {
                'cluster_labels': cluster_labels.tolist(),
                'inertia': kmeans.inertia_
            }
            
        except Exception as e:
            logger.warning(f"èšç±»åˆ†æå¤±è´¥: {e}")
            results['error'] = str(e)
        
        return results

    def perform_anomaly_detection(self, X, features_data):
        """æ‰§è¡Œå¼‚å¸¸æ£€æµ‹"""
        logger.info("  è¿›è¡Œå¼‚å¸¸æ£€æµ‹...")
        
        if X.shape[0] < 10:
            return {}
        
        results = {}
        
        try:
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(X)
            
            normal_count = np.sum(anomaly_labels == 1)
            anomaly_count = np.sum(anomaly_labels == -1)
            
            results['isolation_forest'] = {
                'anomaly_labels': anomaly_labels.tolist(),
                'normal_count': int(normal_count),
                'anomaly_count': int(anomaly_count),
                'anomaly_ratio': float(anomaly_count / len(anomaly_labels))
            }
            
        except Exception as e:
            logger.warning(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            results['error'] = str(e)
        
        return results

    def analyze_feature_importance(self, X, y, feature_names):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        logger.info("  åˆ†æç‰¹å¾é‡è¦æ€§...")
        
        if X.shape[0] < 10:
            return {}
        
        results = {}
        
        try:
            rf = RandomForestRegressor(n_estimators=50, random_state=42)  # å‡å°‘æ ‘çš„æ•°é‡
            rf.fit(X, y)
            
            importances = rf.feature_importances_
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)
            
            results['random_forest_importance'] = importance_df.to_dict('records')
            
        except Exception as e:
            logger.warning(f"ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            results['error'] = str(e)
        
        return results

    def create_comprehensive_visualizations(self, features_data, ml_results):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–"""
        logger.info("åˆ›å»ºç»¼åˆå¯è§†åŒ–...")
        
        try:
            plt.figure(figsize=(20, 16))
            
            # 1. ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
            plt.subplot(3, 3, 1)
            self.plot_feature_correlation(features_data)
            
            # 2. èšç±»ç»“æœå¯è§†åŒ–
            plt.subplot(3, 3, 2)
            self.plot_clustering_results(features_data, ml_results)
            
            # 3. ç‰¹å¾é‡è¦æ€§å›¾
            plt.subplot(3, 3, 3)
            self.plot_feature_importance(ml_results)
            
            # 4. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
            plt.subplot(3, 3, 4)
            self.plot_model_comparison(ml_results)
            
            # 5. æ—¶ç©ºæ¨¡å¼åˆ†æ
            plt.subplot(3, 3, 5)
            self.plot_temporal_patterns(features_data)
            
            # 6. ç»æµæŒ‡æ ‡ä¸æ´»åŠ›æŒ‡æ•°å…³ç³»
            plt.subplot(3, 3, 6)
            self.plot_economic_relationships(features_data)
            
            # 7. å¼‚å¸¸æ£€æµ‹ç»“æœ
            plt.subplot(3, 3, 7)
            self.plot_anomaly_detection(features_data, ml_results)
            
            # 8. POIåˆ†å¸ƒä¸äº¤é€šæµé‡å…³ç³»
            plt.subplot(3, 3, 8)
            self.plot_poi_traffic_relationship(features_data)
            
            # 9. åŒºåŸŸæ´»åŠ›æŒ‡æ•°åœ°å›¾
            plt.subplot(3, 3, 9)
            self.plot_vitality_map(features_data)
            
            plt.tight_layout()
            plot_path = os.path.join(self.output_path, 'comprehensive_analysis_results.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()  # å…³é—­å›¾è¡¨é‡Šæ”¾å†…å­˜
            logger.info(f"âœ… ç»¼åˆå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_path}")
            
            # åˆ›å»ºäº¤äº’å¼åœ°å›¾
            self.create_interactive_map(features_data)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå¯è§†åŒ–æ—¶å‡ºé”™: {e}")

    def plot_feature_correlation(self, features_data):
        """ç»˜åˆ¶ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        try:
            numeric_cols = features_data.select_dtypes(include=[np.number]).columns
            correlation_data = features_data[numeric_cols].corr()
            
            high_corr_features = correlation_data.abs().sum().sort_values(ascending=False).head(10).index
            high_corr_data = correlation_data.loc[high_corr_features, high_corr_features]
            
            sns.heatmap(high_corr_data, annot=True, cmap='coolwarm', center=0,
                       fmt='.2f', linewidths=0.5)
            plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾ (Top 10)', fontsize=12, fontweight='bold')
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
        except Exception as e:
            logger.warning(f"ç›¸å…³æ€§çƒ­åŠ›å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def plot_clustering_results(self, features_data, ml_results):
        """ç»˜åˆ¶èšç±»ç»“æœ"""
        try:
            if 'clustering_analysis' in ml_results and 'kmeans' in ml_results['clustering_analysis']:
                from sklearn.decomposition import PCA
                
                X = features_data.select_dtypes(include=[np.number]).values
                if X.shape[0] > 0 and X.shape[1] > 1:
                    pca = PCA(n_components=2)
                    X_pca = pca.fit_transform(X)
                    
                    cluster_labels = ml_results['clustering_analysis']['kmeans']['cluster_labels']
                    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], 
                                        c=cluster_labels, 
                                        cmap='viridis', alpha=0.7)
                    plt.colorbar(scatter)
                    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
                    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
                    plt.title('èšç±»åˆ†æç»“æœ (PCAé™ç»´)')
                    
        except Exception as e:
            logger.warning(f"èšç±»ç»“æœå¯è§†åŒ–å¤±è´¥: {e}")

    def plot_feature_importance(self, ml_results):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
        try:
            if 'feature_importance' in ml_results and 'random_forest_importance' in ml_results['feature_importance']:
                importance_data = ml_results['feature_importance']['random_forest_importance']
                importance_df = pd.DataFrame(importance_data)
                
                top_features = importance_df.head(10)
                
                plt.barh(range(len(top_features)), top_features['importance'])
                plt.yticks(range(len(top_features)), top_features['feature'])
                plt.xlabel('ç‰¹å¾é‡è¦æ€§')
                plt.title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ (Top 10)')
                plt.gca().invert_yaxis()
                
        except Exception as e:
            logger.warning(f"ç‰¹å¾é‡è¦æ€§å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def plot_model_comparison(self, ml_results):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå›¾"""
        try:
            if 'model_comparison' in ml_results:
                model_scores = {}
                for model_name, scores in ml_results['model_comparison'].items():
                    if 'cv_r2_mean' in scores:
                        model_scores[model_name] = scores['cv_r2_mean']
                
                if model_scores:
                    models = list(model_scores.keys())
                    scores = list(model_scores.values())
                    
                    bars = plt.bar(models, scores)
                    plt.ylabel('äº¤å‰éªŒè¯ RÂ² Score')
                    plt.title('æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ')
                    plt.xticks(rotation=45, ha='right')
                    
                    for bar, score in zip(bars, scores):
                        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                                f'{score:.3f}', ha='center', va='bottom')
                        
        except Exception as e:
            logger.warning(f"æ¨¡å‹æ¯”è¾ƒå›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def plot_temporal_patterns(self, features_data):
        """ç»˜åˆ¶æ—¶é—´æ¨¡å¼åˆ†æå›¾"""
        try:
            if 'hour' in features_data.columns:
                hourly_patterns = features_data.groupby('hour').agg({
                    'volume_mean': 'mean',
                    'speed_mean': 'mean'
                }).reset_index()
                
                fig, ax1 = plt.subplots()
                
                color = 'tab:red'
                ax1.set_xlabel('å°æ—¶')
                ax1.set_ylabel('å¹³å‡æµé‡', color=color)
                ax1.plot(hourly_patterns['hour'], hourly_patterns['volume_mean'], color=color, marker='o')
                ax1.tick_params(axis='y', labelcolor=color)
                
                ax2 = ax1.twinx()
                color = 'tab:blue'
                ax2.set_ylabel('å¹³å‡é€Ÿåº¦', color=color)
                ax2.plot(hourly_patterns['hour'], hourly_patterns['speed_mean'], color=color, marker='s')
                ax2.tick_params(axis='y', labelcolor=color)
                
                plt.title('äº¤é€šæµé‡ä¸é€Ÿåº¦çš„æ—¶é—´æ¨¡å¼')
                fig.tight_layout()
                
        except Exception as e:
            logger.warning(f"æ—¶é—´æ¨¡å¼å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def plot_economic_relationships(self, features_data):
        """ç»˜åˆ¶ç»æµæŒ‡æ ‡å…³ç³»å›¾"""
        try:
            economic_cols = [col for col in features_data.columns if 'economic' in col.lower() or 'gdp' in col.lower()]
            if economic_cols and 'vitality_index' in features_data.columns:
                economic_indicator = economic_cols[0]
                
                plt.scatter(features_data[economic_indicator], features_data['vitality_index'], alpha=0.6)
                plt.xlabel(economic_indicator)
                plt.ylabel('æ´»åŠ›æŒ‡æ•°')
                plt.title('ç»æµæŒ‡æ ‡ä¸æ´»åŠ›æŒ‡æ•°å…³ç³»')
                plt.grid(alpha=0.3)
                
                if len(features_data) > 1:
                    z = np.polyfit(features_data[economic_indicator], features_data['vitality_index'], 1)
                    p = np.poly1d(z)
                    plt.plot(features_data[economic_indicator], p(features_data[economic_indicator]), "r--", alpha=0.8)
                    
        except Exception as e:
            logger.warning(f"ç»æµå…³ç³»å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def plot_anomaly_detection(self, features_data, ml_results):
        """ç»˜åˆ¶å¼‚å¸¸æ£€æµ‹ç»“æœ"""
        try:
            if 'anomaly_detection' in ml_results and 'isolation_forest' in ml_results['anomaly_detection']:
                anomaly_data = ml_results['anomaly_detection']['isolation_forest']
                
                feature1, feature2 = 'volume_mean', 'poi_density'
                if feature1 in features_data.columns and feature2 in features_data.columns:
                    
                    colors = ['red' if label == -1 else 'blue' for label in anomaly_data['anomaly_labels']]
                    
                    plt.scatter(features_data[feature1], features_data[feature2], 
                               c=colors, alpha=0.6)
                    plt.xlabel(feature1)
                    plt.ylabel(feature2)
                    plt.title('å¼‚å¸¸æ£€æµ‹ç»“æœ\n(çº¢è‰²=å¼‚å¸¸ç‚¹)')
                    plt.grid(alpha=0.3)
                    
        except Exception as e:
            logger.warning(f"å¼‚å¸¸æ£€æµ‹å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def plot_poi_traffic_relationship(self, features_data):
        """ç»˜åˆ¶POIä¸äº¤é€šæµé‡å…³ç³»å›¾"""
        try:
            if 'poi_density' in features_data.columns and 'volume_mean' in features_data.columns:
                plt.scatter(features_data['poi_density'], features_data['volume_mean'], alpha=0.6)
                plt.xlabel('POIå¯†åº¦ (ä¸ª/å¹³æ–¹å…¬é‡Œ)')
                plt.ylabel('å¹³å‡äº¤é€šæµé‡ (è½¦è¾†/å°æ—¶)')
                plt.title('POIå¯†åº¦ vs äº¤é€šæµé‡')
                plt.grid(alpha=0.3)
                
                if len(features_data) > 1:
                    z = np.polyfit(features_data['poi_density'], features_data['volume_mean'], 1)
                    p = np.poly1d(z)
                    plt.plot(features_data['poi_density'], p(features_data['poi_density']), "r--", alpha=0.8)
                    
        except Exception as e:
            logger.warning(f"POI-äº¤é€šå…³ç³»å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def plot_vitality_map(self, features_data):
        """ç»˜åˆ¶åŒºåŸŸæ´»åŠ›æŒ‡æ•°åœ°å›¾"""
        try:
            if 'vitality_index' in features_data.columns and 'district' in features_data.columns:
                district_vitality = features_data.groupby('district')['vitality_index'].mean().sort_values(ascending=False)
                
                plt.bar(range(len(district_vitality)), district_vitality.values)
                plt.xticks(range(len(district_vitality)), district_vitality.index, rotation=45, ha='right')
                plt.ylabel('å¹³å‡æ´»åŠ›æŒ‡æ•°')
                plt.title('å„åŒºåŸŸæ´»åŠ›æŒ‡æ•°åˆ†å¸ƒ')
                plt.grid(axis='y', alpha=0.3)
                
        except Exception as e:
            logger.warning(f"æ´»åŠ›æŒ‡æ•°åœ°å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    def create_interactive_map(self, features_data):
        """åˆ›å»ºäº¤äº’å¼åœ°å›¾"""
        try:
            hk_center = [22.3193, 114.1694]
            m = folium.Map(location=hk_center, zoom_start=11, tiles='OpenStreetMap')
            
            if 'vitality_index' in features_data.columns and 'district' in features_data.columns:
                district_vitality = features_data.groupby('district')['vitality_index'].mean().reset_index()
                
                district_coords = {
                    'Central and Western': [22.2866, 114.1550],
                    'Wan Chai': [22.2796, 114.1729],
                    'Eastern': [22.2841, 114.2191],
                    'Southern': [22.2476, 114.1584],
                    'Yau Tsim Mong': [22.3195, 114.1694],
                    'Sham Shui Po': [22.3307, 114.1625],
                    'Kowloon City': [22.3282, 114.1911],
                    'Wong Tai Sin': [22.3425, 114.1929],
                    'Kwun Tong': [22.3124, 114.2254],
                    'Kwai Tsing': [22.3544, 114.1220],
                    'Tsuen Wan': [22.3707, 114.1118],
                    'Tuen Mun': [22.3915, 113.9725],
                    'Yuen Long': [22.4454, 114.0221],
                    'North': [22.4942, 114.1384],
                    'Tai Po': [22.4504, 114.1612],
                    'Sha Tin': [22.3809, 114.1869],
                    'Sai Kung': [22.3829, 114.2704],
                    'Islands': [22.2615, 113.9466]
                }
                
                for _, row in district_vitality.iterrows():
                    district = row['district']
                    vitality = row['vitality_index']
                    
                    if district in district_coords:
                        coord = district_coords[district]
                        
                        if vitality >= 80:
                            color = 'green'
                        elif vitality >= 60:
                            color = 'lightgreen'
                        elif vitality >= 40:
                            color = 'orange'
                        elif vitality >= 20:
                            color = 'lightred'
                        else:
                            color = 'red'
                        
                        folium.Marker(
                            location=coord,
                            popup=f"{district}<br>æ´»åŠ›æŒ‡æ•°: {vitality:.1f}",
                            tooltip=district,
                            icon=folium.Icon(color=color, icon='info-sign')
                        ).add_to(m)
            
            map_path = os.path.join(self.output_path, 'interactive_vitality_map.html')
            m.save(map_path)
            logger.info(f"âœ… äº¤äº’å¼åœ°å›¾å·²ä¿å­˜: {map_path}")
            
        except Exception as e:
            logger.warning(f"äº¤äº’å¼åœ°å›¾åˆ›å»ºå¤±è´¥: {e}")

    def generate_ultimate_report(self, features_data, ml_results, processing_time, traffic_records, restaurant_records):
        """ç”Ÿæˆç»ˆæåˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»ˆæåˆ†ææŠ¥å‘Š...")
        
        total_records = len(features_data)
        districts_covered = features_data['district'].nunique() if 'district' in features_data.columns else 0
        
        vitality_stats = {
            'mean': features_data['vitality_index'].mean() if 'vitality_index' in features_data.columns else 0,
            'std': features_data['vitality_index'].std() if 'vitality_index' in features_data.columns else 0,
            'max': features_data['vitality_index'].max() if 'vitality_index' in features_data.columns else 0,
            'min': features_data['vitality_index'].min() if 'vitality_index' in features_data.columns else 0
        }
        
        model_performance = ""
        if 'model_comparison' in ml_results:
            for model_name, scores in ml_results['model_comparison'].items():
                if 'cv_r2_mean' in scores:
                    model_performance += f"    - {model_name}: RÂ² = {scores['cv_r2_mean']:.3f} (Â±{scores['cv_r2_std']:.3f})\n"
        
        top_features = ""
        if 'feature_importance' in ml_results and 'random_forest_importance' in ml_results['feature_importance']:
            importance_data = ml_results['feature_importance']['random_forest_importance']
            for i, item in enumerate(importance_data[:3]):
                top_features += f"    - {item['feature']}: {item['importance']:.3f}\n"
        
        report = f"""
        ===============================================
        é¦™æ¸¯POIä¸äº¤é€šæµé‡ç»ˆæä¼˜åŒ–åˆ†ææŠ¥å‘Š
        ï¼ˆå†…å­˜ä¼˜åŒ– + å¤æ‚ç‰¹å¾å·¥ç¨‹ + å…¨é¢åˆ†æï¼‰
        ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        ===============================================
        
        æ‰§è¡Œæ¦‚è§ˆ:
        ---------
        - æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’ ({processing_time/60:.2f} åˆ†é’Ÿ)
        - å†…å­˜å³°å€¼ä½¿ç”¨: {self.get_memory_usage()['percent']:.1f}%
        - äº¤é€šæ•°æ®è®°å½•: {traffic_records:,}
        - é¤å…æ•°æ®è®°å½•: {restaurant_records:,}
        - åˆ†æè®°å½•æ•°: {total_records:,}
        - è¦†ç›–åŒºåŸŸæ•°: {districts_covered}
        
        æ•°æ®ç‰¹å¾:
        ---------
        - ç‰¹å¾æ€»æ•°: {len(features_data.columns)}
        - æ•°å€¼ç‰¹å¾: {len(features_data.select_dtypes(include=[np.number]).columns)}
        - åˆ†ç±»ç‰¹å¾: {len(features_data.select_dtypes(include=['object']).columns)}
        
        æ´»åŠ›æŒ‡æ•°åˆ†æ:
        -------------
        - å¹³å‡æ´»åŠ›æŒ‡æ•°: {vitality_stats['mean']:.2f}
        - æ ‡å‡†å·®: {vitality_stats['std']:.2f}
        - æœ€é«˜å€¼: {vitality_stats['max']:.2f}
        - æœ€ä½å€¼: {vitality_stats['min']:.2f}
        
        æœºå™¨å­¦ä¹ åˆ†æç»“æœ:
        -----------------
        æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:
        {model_performance if model_performance else '    - æ— å¯ç”¨æ¨¡å‹ç»“æœ'}
        
        æœ€é‡è¦çš„3ä¸ªç‰¹å¾:
        {top_features if top_features else '    - æ— ç‰¹å¾é‡è¦æ€§æ•°æ®'}
        
        æŠ€æœ¯ç‰¹ç‚¹:
        ---------
        - âœ… å†…å­˜ä¼˜åŒ–: ä½¿ç”¨Daskå¹¶è¡Œå¤„ç† + æ•°æ®åˆ†å— + å†…å­˜ç›‘æ§
        - âœ… å¤æ‚ç‰¹å¾å·¥ç¨‹: {len(features_data.columns)}ä¸ªç‰¹å¾ç»´åº¦
        - âœ… çœŸå®æ•°æ®: åŸºäºé¦™æ¸¯å®˜æ–¹è¡Œæ”¿åŒºåˆ’å’Œç»æµæ•°æ®
        - âœ… æ— æŠ½æ ·: ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œåˆ†æ
        - âœ… é«˜çº§åˆ†æ: èšç±»ã€å¼‚å¸¸æ£€æµ‹ã€ç‰¹å¾é‡è¦æ€§
        - âœ… äº¤äº’å¼å¯è§†åŒ–: ç”Ÿæˆäº¤äº’å¼åœ°å›¾å’Œå¤šç§å›¾è¡¨
        
        å…³é”®å‘ç°:
        ---------
        1. å†…å­˜ä¼˜åŒ–æ•ˆæœ:
           - æˆåŠŸå¤„ç†è¶…å¤§è§„æ¨¡æ•°æ®è€Œä¸å‡ºç°å†…å­˜ä¸è¶³
           - æ™ºèƒ½å†…å­˜ç›‘æ§å’Œè‡ªåŠ¨åƒåœ¾å›æ”¶
           - æ•°æ®åˆ†å—å¤„ç†å’Œå¹¶è¡Œè®¡ç®—
        
        2. ç‰¹å¾å·¥ç¨‹æ·±åº¦:
           - åˆ›å»ºäº†{len(features_data.columns)}ä¸ªç‰¹å¾ç»´åº¦
           - åŒ…å«äº¤é€šæ¨¡å¼ã€POIåˆ†å¸ƒã€ç»æµæŒ‡æ ‡ç­‰å¤šç»´åº¦ç‰¹å¾
           - å®ç°äº†æ—¶ç©ºç‰¹å¾çš„ç»¼åˆåˆ†æ
        
        3. æœºå™¨å­¦ä¹ æ´å¯Ÿ:
           - å¤šä¸ªæ¨¡å‹åœ¨é¢„æµ‹æ´»åŠ›æŒ‡æ•°æ–¹é¢è¡¨ç°è‰¯å¥½
           - ç‰¹å¾é‡è¦æ€§åˆ†ææ­ç¤ºäº†å…³é”®å½±å“å› ç´ 
           - å‘ç°äº†ä¸åŒçš„åŒºåŸŸå‘å±•æ¨¡å¼
        
        4. ç©ºé—´åˆ†æä»·å€¼:
           - æ­ç¤ºäº†POIåˆ†å¸ƒä¸äº¤é€šæµé‡çš„ç©ºé—´ç›¸å…³æ€§
           - è¯†åˆ«äº†é«˜æ´»åŠ›å’Œä½æ´»åŠ›åŒºåŸŸçš„ç©ºé—´æ¨¡å¼
           - ä¸ºåŸå¸‚è§„åˆ’æä¾›äº†æ•°æ®æ”¯æŒ
        
        ===============================================
        """
        
        report_path = os.path.join(self.output_path, 'ultimate_analysis_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ… ç»ˆæåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print(report)

    def save_ultimate_results(self, traffic_data, restaurant_data, features_data, ml_results):
        """ä¿å­˜ç»ˆæåˆ†æç»“æœ"""
        logger.info("ä¿å­˜ç»ˆæåˆ†æç»“æœ...")
        
        # ä¿å­˜ç‰¹å¾æ•°æ®
        features_path = os.path.join(self.output_path, 'ultimate_features_data.csv')
        features_data.to_csv(features_path, index=False, encoding='utf-8-sig')
        logger.info(f"  ç‰¹å¾æ•°æ®å·²ä¿å­˜: {features_path}")
        
        # ä¿å­˜æœºå™¨å­¦ä¹ ç»“æœ
        ml_results_path = os.path.join(self.output_path, 'ultimate_ml_results.json')
        with open(ml_results_path, 'w', encoding='utf-8') as f:
            json.dump(ml_results, f, indent=2, ensure_ascii=False)
        logger.info(f"  æœºå™¨å­¦ä¹ ç»“æœå·²ä¿å­˜: {ml_results_path}")
        
        # ä¿å­˜æ•°æ®ç»Ÿè®¡
        stats_path = os.path.join(self.output_path, 'ultimate_data_statistics.txt')
        with open(stats_path, 'w', encoding='utf-8') as f:
            f.write("ç»ˆæä¼˜åŒ–ç‰ˆæ•°æ®åˆ†æç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"åˆ†æå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"äº¤é€šæ•°æ®è®°å½•æ•°: {len(traffic_data) if hasattr(traffic_data, '__len__') else 'N/A':,}\n")
            f.write(f"é¤å…æ•°æ®è®°å½•æ•°: {len(restaurant_data) if hasattr(restaurant_data, '__len__') else 'N/A':,}\n")
            f.write(f"ç‰¹å¾æ•°æ®è®°å½•æ•°: {len(features_data) if not features_data.empty else 0:,}\n")
            f.write(f"ç‰¹å¾ç»´åº¦: {len(features_data.columns) if not features_data.empty else 0}\n")
            f.write(f"è¦†ç›–åŒºåŸŸ: {features_data['district'].nunique() if 'district' in features_data.columns else 0}\n")
            f.write(f"å†…å­˜ä½¿ç”¨å³°å€¼: {self.get_memory_usage()['percent']:.1f}%\n")
        
        logger.info(f"  æ•°æ®ç»Ÿè®¡å·²ä¿å­˜: {stats_path}")

    def run_ultimate_analysis(self):
        """è¿è¡Œç»ˆæä¼˜åŒ–åˆ†ææµç¨‹"""
        logger.info("=" * 80)
        logger.info("ğŸš€ é¦™æ¸¯POIä¸äº¤é€šæµé‡ç»ˆæä¼˜åŒ–åˆ†æç³»ç»Ÿ")
        logger.info("     å†…å­˜ä¼˜åŒ– + å¤æ‚ç‰¹å¾å·¥ç¨‹ + å…¨é¢åˆ†æ")
        logger.info("=" * 80)
        
        start_time = time.time()
        initial_memory = self.get_memory_usage()
        logger.info(f"åˆå§‹å†…å­˜: {initial_memory['percent']:.1f}% ({initial_memory['available_gb']:.1f} GB å¯ç”¨)")
        
        try:
            # 1. ä¼˜åŒ–æ•°æ®åŠ è½½
            logger.info("\n1. ğŸ“¥ ä¼˜åŒ–æ•°æ®åŠ è½½é˜¶æ®µ...")
            traffic_data = self.load_traffic_data_optimized()
            restaurant_data = self.load_restaurant_data_optimized()
            
            traffic_records = len(traffic_data) if hasattr(traffic_data, '__len__') else 'èšåˆæ•°æ®'
            restaurant_records = len(restaurant_data) if hasattr(restaurant_data, '__len__') else 'N/A'
            
            logger.info(f"äº¤é€šæ•°æ®: {traffic_records:,} æ¡è®°å½•")
            logger.info(f"é¤å…æ•°æ®: {restaurant_records:,} æ¡è®°å½•")
            
            # 2. é«˜çº§ç‰¹å¾å·¥ç¨‹
            logger.info("\n2. ğŸ› ï¸ é«˜çº§ç‰¹å¾å·¥ç¨‹é˜¶æ®µ...")
            features_data = self.advanced_feature_engineering(traffic_data, restaurant_data)
            
            # 3. æœºå™¨å­¦ä¹ åˆ†æ
            logger.info("\n3. ğŸ¤– æœºå™¨å­¦ä¹ åˆ†æé˜¶æ®µ...")
            ml_results = self.advanced_ml_analysis(features_data)
            
            # 4. é«˜çº§å¯è§†åŒ–
            logger.info("\n4. ğŸ¨ é«˜çº§å¯è§†åŒ–é˜¶æ®µ...")
            self.create_comprehensive_visualizations(features_data, ml_results)
            
            # 5. æŠ¥å‘Šç”Ÿæˆ
            logger.info("\n5. ğŸ“Š æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ...")
            total_time = time.time() - start_time
            self.generate_ultimate_report(features_data, ml_results, total_time, 
                                        traffic_records, restaurant_records)
            
            # 6. æ•°æ®ä¿å­˜
            logger.info("\n6. ğŸ’¾ æ•°æ®ä¿å­˜é˜¶æ®µ...")
            self.save_ultimate_results(traffic_data, restaurant_data, features_data, ml_results)
            
            final_memory = self.get_memory_usage()
            
            logger.info(f"\n" + "=" * 70)
            logger.info("ğŸ‰ ç»ˆæä¼˜åŒ–åˆ†ææˆåŠŸå®Œæˆ!")
            logger.info(f"æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
            logger.info(f"æœ€ç»ˆå†…å­˜: {final_memory['percent']:.1f}%")
            logger.info(f"å†…å­˜å˜åŒ–: {final_memory['used_gb'] - initial_memory['used_gb']:+.2f} GB")
            logger.info(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.output_path}")
            logger.info("=" * 70)
            
            return True
            
        except Exception as e:
            logger.error(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # é…ç½®è·¯å¾„
    traffic_base_path = r"E:\transport volume"
    restaurant_data_path = r"E:\Restaurant Licence"
    output_path = r"E:\HK_POI_Traffic_Ultimate_Analysis_Results"
    
    print("åˆå§‹åŒ–é¦™æ¸¯POIä¸äº¤é€šæµé‡ç»ˆæä¼˜åŒ–åˆ†æç³»ç»Ÿ...")
    print("æ­¤ç‰ˆæœ¬è§£å†³å†…å­˜é—®é¢˜ï¼ŒåŒ…å«å¤æ‚ç‰¹å¾å·¥ç¨‹å’Œå…¨é¢åˆ†æ")
    print("åŸºäºå®Œæ•´çœŸå®æ•°æ®ï¼Œæ— æŠ½æ ·")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = HK_POI_Traffic_Ultimate_Analysis(
        traffic_base_path=traffic_base_path,
        restaurant_data_path=restaurant_data_path,
        output_path=output_path
    )
    
    # è¿è¡Œç»ˆæåˆ†æ
    success = analyzer.run_ultimate_analysis()
    
    if success:
        print("\nğŸ‰ ç»ˆæä¼˜åŒ–åˆ†ææˆåŠŸå®Œæˆ!")
        print(f"è¯·æŸ¥çœ‹è¾“å‡ºç›®å½•: {output_path}")
        print("\nç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
        print("  - comprehensive_analysis_results.png (ç»¼åˆåˆ†æå›¾è¡¨)")
        print("  - interactive_vitality_map.html (äº¤äº’å¼åœ°å›¾)")
        print("  - ultimate_analysis_report.txt (ç»ˆæåˆ†ææŠ¥å‘Š)")
        print("  - ultimate_features_data.csv (ç‰¹å¾æ•°æ®)")
        print("  - ultimate_ml_results.json (æœºå™¨å­¦ä¹ ç»“æœ)")
        print("  - ultimate_data_statistics.txt (æ•°æ®ç»Ÿè®¡)")
    else:
        print("\nâŒ åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜")
        print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®")

if __name__ == "__main__":
    main()